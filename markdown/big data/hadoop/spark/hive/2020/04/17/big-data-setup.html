<p>Working on your own data science projects are a great opportunity to learn some new skills and hone existing skills, but what if you want to use technologies that you would use in industry such as Hadoop, Spark on a distributed cluster, Hive, etc. and have them all integrated? This is where the value comes from when building your own infrastructure.</p>

<p>You become familiar with the technologies, get to know the ins and outs about how it operates, debug and experience the different types of error messages and really get a sense of how the technology works over all instead of just interfacing with it. If you are also working with your own private data or confidential data in general, you may not want to upload it to an external service to do big data processing for privacy or security reasons. So, in this tutorial I’m going to walk through how to setup your own Big Data infrastructure on your own computer, home lab, etc. We’re going to setup a single node Hadoop &amp; Hive instance and a “distributed” spark cluster integrated with Jupyter.</p>

<p><em>Edit</em>: Thanks to <a href="https://medium.com/@dvillaj">@Daniel Villanueva</a> you can now deploy a VM with Hadoop, Spark and Hive pre-configured and ready to go through his Vagrant image. You can check it out on his Github <a href="https://github.com/dvillaj/spark-box">here</a>.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">This tutorial is not for an industry production installation!</span>
</div>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>A Debian based distro - Ubuntu, Pop-os, etc</li>
  <li>Basic command line knowledge helps, but not essential for installation</li>
</ul>

<h2 id="step-1---download-hadoop-and-hive">Step 1 - Download Hadoop and Hive</h2>

<p>Hadoop is easily the most common big data warehouse platform used in industry today and is a must know for any big data job. In short, Hadoop is an open-source software framework used for storing and processing Big Data in a distributed manner. You can download the latest version from <a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.3/hadoop-3.1.3-src.tar.gz">here</a>.</p>

<p>Hive is usually added on top of Hadoop to query the data in Hadoop in a SQL like fashion. Hive makes job easy for performing operations like</p>

<ul>
  <li>Data encapsulation</li>
  <li>Ad-hoc queries</li>
  <li>Analysis of huge datasets</li>
</ul>

<p>Hive is slow and generally used for batch jobs only. A much faster version of Hive would be something like Impala, but for home use - it gets the job done. You can download the latest version of Hive <a href="https://downloads.apache.org/hive/hive-3.1.2/">here</a>.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">Make sure you download the binary (bin) version and not the source (src) version!</span>
</div>

<h4 id="extract-the-files-to-opt">Extract the files to /opt</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/Downloads
<span class="nb">tar</span> <span class="nt">-C</span> /opt <span class="nt">-xzvf</span> apache-hive-3.1.2-bin.tar.gz
<span class="nb">tar</span> <span class="nt">-C</span> /opt <span class="nt">-xzvf</span> hadoop-3.1.3-src.tar.gz
</code></pre></div></div>

<p>Rename them to <code class="language-plaintext highlighter-rouge">hive</code> and <code class="language-plaintext highlighter-rouge">hadoop</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /opt
<span class="nb">mv </span>hadoop-3.1.3-src hadoop
<span class="nb">mv </span>apache-hive-3.1.2-bin hive
</code></pre></div></div>

<h2 id="step-2---setup-authorized-or-password-less-ssh">Step 2 - Setup Authorized (or Password-less) SSH.</h2>

<p>Why do we need to do this? The Hadoop core uses Shell (SSH) to launch the server processes on the slave nodes. It requires a password-less SSH connection between the master and all conencted nodes. Otherwise, you would have to manually go to each node and start each Hadoop process.</p>

<p>Since we are running a local instance of Hadoop, we can save ourselves the hassle of setting up hostnames, SSH keys and adding them to each box. If this were a distributed environment, it would also be best to create a <code class="language-plaintext highlighter-rouge">hadoop</code> user, but it’s not necessary for a single node setup and personal use.</p>

<p>The really easy, <strong>only suitable for home use, should not be used or done anywhere else way is</strong>:</p>

<p><code class="language-plaintext highlighter-rouge">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p>

<p>Now run <code class="language-plaintext highlighter-rouge">ssh localhost</code> and you should be able to login without a password.</p>

<p><img src="/blog/images/bigdata_tut/ssh.png" alt="" title="Successful SSH login" /></p>

<p>To get an idea of what it takes to configure the networking and infrastructure on a distributed environment,  <a href="https://www.tutorialspoint.com/hadoop/hadoop_multi_node_cluster.htm">this</a> is a great source.</p>

<h2 id="step-3---install-java-8">Step 3 - Install Java 8</h2>

<p>One of the most important steps of this tutorial.</p>

<div class="Toast Toast--warning googoo" style="max-width: 1200px;">
   <span class="Toast-icon"><svg class="octicon octicon-alert octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg></span>
   <span class="Toast-content">If this is done incorrectly, it will cause a grueling number of hours debugging vague error messages just to realize the problem and solution was so simple.</span>
</div>

<p>Hadoop has one main requirement and this is Java version 8. Funnily enough, that’s also the Java requirement for Spark, also very important.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install </span>openjdk-8-jdk
</code></pre></div></div>

<p>Verify the Java version.</p>

<p><code class="language-plaintext highlighter-rouge">java -version</code></p>

<p><img src="/blog/images/bigdata_tut/corr_java.png" alt="" title="Incorrect Java version" /></p>

<p>If for some reason you don’t see the output above, you need to update your default Java version.</p>

<p><code class="language-plaintext highlighter-rouge">sudo update-alternatives --config java</code></p>

<p><img src="/blog/images/bigdata_tut/update-alt.png" alt="" title="Update Java version" /></p>

<p>Choose the number associated with Java 8.</p>

<p>Check the version again.</p>

<p><code class="language-plaintext highlighter-rouge">java -version</code></p>

<p><img src="/blog/images/bigdata_tut/corr_java.png" alt="" title="Correct Java version" /></p>

<h2 id="step-4---configure-hadoop--yarn">Step 4 - Configure Hadoop + Yarn</h2>

<p>Apache Hadoop YARN (Yet Another Resource Negotiator) is a cluster management technology. At a very basic level it helps Hadoop manage and monitor its workloads.</p>

<h4 id="initial-hadoop-setup">Initial Hadoop Setup</h4>

<p>First let’s set our environment variables. These specifies where the configuration for Hadoop, Spark and Hive is located.</p>

<p><code class="language-plaintext highlighter-rouge">nano ~/.bashrc</code></p>

<p>Add this to the bottom of your <code class="language-plaintext highlighter-rouge">.bashrc</code> file.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/opt/hadoop
<span class="nb">export </span><span class="nv">HADOOP_INSTALL</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">YARN_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_LIB_NATIVE_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/lib/native
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HADOOP_HOME</span>/sbin:<span class="nv">$HADOOP_HOME</span>/bin

<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/lib/native:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="nb">export </span><span class="nv">HIVE_HOME</span><span class="o">=</span>/opt/hive
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HIVE_HOME</span>/bin
</code></pre></div></div>

<p>Save and exit out of nano <code class="language-plaintext highlighter-rouge">CTRL + o</code>, <code class="language-plaintext highlighter-rouge">CTRL + x</code>.</p>

<p>Then we need to active these changes by running <code class="language-plaintext highlighter-rouge">source ~/.bashrc</code>. You can also close and reopen your terminal to achieve the same result.</p>

<p>Next we need to make some directories and edit permissions. Make the following directories:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /app/hadoop/tmp
<span class="nb">mkdir</span> <span class="nt">-p</span> ~/hdfs/namenode
<span class="nb">mkdir</span> ~/hdfs/datanode
</code></pre></div></div>

<p>Edit the permissions for <code class="language-plaintext highlighter-rouge">/app/hadoop/tmp</code>, giving it read and write access.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chown</span> <span class="nt">-R</span> <span class="nv">$USER</span>:<span class="nv">$USER</span> /app
<span class="nb">chmod </span>a+rw <span class="nt">-R</span> /app
</code></pre></div></div>

<h4 id="config-files">Config Files</h4>

<p>All the Hadoop configuration files are located in <code class="language-plaintext highlighter-rouge">/opt/hadoop/etc/hadoop/</code>.</p>

<p><code class="language-plaintext highlighter-rouge">cd /opt/hadoop/etc/hadoop</code></p>

<p>Next we need to edit the following configuration files:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- core-site.xml
- hadoop-env.sh
- hdfs-site.xml
- mapred-site.xml
- yarn-site.xml
</code></pre></div></div>

<p><strong>core-site.xml</strong></p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>/app/hadoop/tmp<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;description&gt;</span>Parent directory for other temporary directories.<span class="nt">&lt;/description&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>fs.defaultFS <span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>hdfs://YOUR_IP:9000<span class="nt">&lt;/value&gt;</span>
		<span class="nt">&lt;description&gt;</span>The name of the default file system. <span class="nt">&lt;/description&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">hadoop.tmp.dir</code>: Fairly self explanatory, just a directory for hadoop to store other temp directories
<code class="language-plaintext highlighter-rouge">fs.defaultFS</code>: The IP and port of your file system to access over the network. It should be your IP so other nodes can connect to it if this were a distributed system.</p>

<p>To find your ip, type <code class="language-plaintext highlighter-rouge">ip addr</code> or <code class="language-plaintext highlighter-rouge">ifconfig</code> on the command line:</p>

<p><img src="/blog/images/bigdata_tut/ip.png" alt="" title="Find your IP" /></p>

<p><strong>hadoop-env.sh</strong></p>

<ol>
  <li>Identify the location of the Java 8 JDK, it should be similar or idential to <code class="language-plaintext highlighter-rouge">/usr/lib/jvm/java-8-openjdk-amd64/</code></li>
  <li>Add the following line to <code class="language-plaintext highlighter-rouge">hadoop-env.sh</code>: <code class="language-plaintext highlighter-rouge">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/</code></li>
</ol>

<p><strong>hdfs-site.xml</strong></p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>1<span class="nt">&lt;/value&gt;</span>
		<span class="nt">&lt;description&gt;</span>Default block replication.<span class="nt">&lt;/description&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.name.dir<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>file:///home/YOUR_USER/hdfs/namenode<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>dfs.data.dir<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>file:///home/YOUR_USER/hdfs/datanode<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">dfs.replication</code>: How many nodes to replicate the data on.</p>

<p><code class="language-plaintext highlighter-rouge">dfs.name.dir</code>: Directory for namenode blocks</p>

<p><code class="language-plaintext highlighter-rouge">dfs.data.dir</code>: Directory for the data node blocks</p>

<p><strong>mapred-site.xml</strong></p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>mapreduce.jobtracker.address<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>localhost:54311<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.env<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>mapreduce.map.env<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>mapreduce.reduce.env<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
    	<span class="nt">&lt;name&gt;</span>mapreduce.map.memory.mb<span class="nt">&lt;/name&gt;</span>
    	<span class="nt">&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
    	<span class="nt">&lt;name&gt;</span>mapreduce.reduce.memory.mb<span class="nt">&lt;/name&gt;</span>
    	<span class="nt">&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">mapreduce.framework.name</code>: The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.</p>

<p><code class="language-plaintext highlighter-rouge">mapreduce.jobtracker.address</code>: The host and port that the MapReduce job tracker runs at. If “local”, then jobs are run in-process as a single map and reduce task.</p>

<p><code class="language-plaintext highlighter-rouge">yarn.app.mapreduce.am.env</code>: Yarn map reduce env variable.</p>

<p><code class="language-plaintext highlighter-rouge">mapreduce.map.env</code>: Map reduce map env variable.</p>

<p><code class="language-plaintext highlighter-rouge">mapreduce.reduce.env</code>: Map reduce reduce env variable.</p>

<p><code class="language-plaintext highlighter-rouge">mapreduce.map.memory.mb</code>: Upper memory limit that Hadoop allows to be allocated to a mapper, in megabytes. The default is 512.</p>

<p><code class="language-plaintext highlighter-rouge">mapreduce.reduce.memory.mb</code>: Upper memory limit that Hadoop allows to be allocated to a reducer, in megabytes. The default is 512.</p>

<p><strong>yarn-site.xml</strong></p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
<span class="c">&lt;!-- Site specific YARN configuration properties --&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>localhost<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.nodemanager.resource.memory-mb<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>16256<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
	<span class="nt">&lt;property&gt;</span>
		<span class="nt">&lt;name&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="nt">&lt;/name&gt;</span>
		<span class="nt">&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;</span>
	<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">yarn.resourcemanager.hostname</code>: The hostname of the RM. Could also be an ip address of a remote yarn instance.</p>

<p><code class="language-plaintext highlighter-rouge">yarn.nodemanager.aux-services</code>: Selects a shuffle service that needs to be set for MapReduce to run.</p>

<p><code class="language-plaintext highlighter-rouge">yarn.nodemanager.resource.memory-mb</code>: Amount of physical memory, in MB, that can be allocated for containers. For reference, I have 64GB of RAM on my machine. If this value is too low, you won’t be able to process large files, getting a <code class="language-plaintext highlighter-rouge">FileSegmentManagedBuffer</code> error.</p>

<p><code class="language-plaintext highlighter-rouge">yarn.app.mapreduce.am.resource.mb</code>: This property specify criteria to select resource for particular job. Any nodemanager which has equal or more memory available will get selected for executing job.</p>

<p><code class="language-plaintext highlighter-rouge">yarn.scheduler.minimum-allocation-mb</code>: The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this won’t take effect, and the specified value will get allocated at minimum.</p>

<h4 id="start-hadoop">Start Hadoop</h4>

<p>Before we start Hadoop we have to format the namenode:</p>

<p><code class="language-plaintext highlighter-rouge">hdfs namenode -format</code></p>

<p>Now we’re good to start Hadoop! Run the following commands:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>start-dfs.sh
start-yarn.sh
</code></pre></div></div>

<p>To ensure everything has started run the following commands:</p>

<p><code class="language-plaintext highlighter-rouge">ss -ln | grep 9000</code></p>

<p><img src="/blog/images/bigdata_tut/ss.png" alt="" title="ss output" /></p>

<p><code class="language-plaintext highlighter-rouge">jps</code></p>

<p><img src="/blog/images/bigdata_tut/jps.png" alt="" title="jps output" /></p>

<p>You can now also access the Hadoop web UI at localhost:9870.</p>

<p><img src="/blog/images/bigdata_tut/hadoop.png" alt="" title="Hadoop web UI" /></p>

<p>You can also access the Yarn web UI at localhost:8088.</p>

<p><img src="/blog/images/bigdata_tut/yarn.png" alt="" title="Yarn web UI" /></p>

<h2 id="step-5---setup-hive">Step 5 - Setup Hive</h2>

<p>Now that we have Hadoop up and running, let’s install Hive on top of it.</p>

<p>First let’s make a directory in Hadoop where our Hive tables are going to be stored.</p>

<p><code class="language-plaintext highlighter-rouge">hdfs dfs -mkdir -p /user/hive/warehouse</code></p>

<p>Configure permissions.</p>

<p><code class="language-plaintext highlighter-rouge">hdfs dfs -chmod -R a+rw /user/hive</code></p>

<h4 id="setup-a-metastore">Setup a Metastore</h4>

<p>The Hive Metastore is the central repository of Hive Metadata. It stores the meta data for Hive tables and relations (Schema and Locations etc). It provides client access to this information by using metastore service API. There are 3 different types of metastores:</p>

<ul>
  <li>Embedded Metastore: Only one Hive session can be open at a time.</li>
  <li>Local Metastore: Multiple Hive sessions, have to connect to an external DB.</li>
  <li>Remote Metastore: Multiple Hive sessions, interact with the metastore using Thrift API, better security and scalability.</li>
</ul>

<p>To read up on the difference between each type of metastore in more detail, this is a great <a href="https://data-flair.training/blogs/apache-hive-metastore/">link</a>.</p>

<p>In this guide we’re going to be setting up a remote metastore using a MySQL DB.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>mysql-server
<span class="nb">sudo </span>mysql_secure_installation
</code></pre></div></div>

<p>Run the following commands:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>mysql
</code></pre></div></div>

<pre><code class="language-SQL">CREATE DATABASE metastore;
CREATE USER 'hive'@'%' IDENTIFIED BY 'PW_FOR_HIVE';
GRANT ALL ON metastore.* TO 'hive'@'%' WITH GRANT OPTION;
</code></pre>

<p>Replace <code class="language-plaintext highlighter-rouge">PW_FOR_HIVE</code> with the password you want to use for the hive user in MySQL.</p>

<p>Download the MySQL Java Connector:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19.tar.gz
<span class="nb">tar</span> <span class="nt">-xzvf</span> mysql-connector-java-8.0.19.tar.gz
<span class="nb">cd </span>mysql-connect-java-8.0.19
<span class="nb">cp </span>mysql-connector-java-8.0.19.jar /opt/hive/lib/
</code></pre></div></div>

<h4 id="edit-hive-sitexml">Edit hive-site.xml</h4>

<p>Now edit <code class="language-plaintext highlighter-rouge">/opt/hive/conf/hive-site.xml</code>:</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>jdbc:mysql://YOUR_IP:3306/metastore?createDatabaseIfNotExist=true<span class="ni">&amp;amp;</span>useLegacyDatetimeCode=false<span class="ni">&amp;amp;</span>serverTimezone=UTC<span class="nt">&lt;/value&gt;</span>
                <span class="nt">&lt;description&gt;</span>metadata is stored in a MySQL server<span class="nt">&lt;/description&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionDriverName<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>com.mysql.jdbc.Driver<span class="nt">&lt;/value&gt;</span>
                <span class="nt">&lt;description&gt;</span>MySQL JDBC driver class<span class="nt">&lt;/description&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionUserName<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hive<span class="nt">&lt;/value&gt;</span>
                <span class="nt">&lt;description&gt;</span>user name for connecting to mysql server<span class="nt">&lt;/description&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionPassword<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>PW_FOR_HIVE<span class="nt">&lt;/value&gt;</span>
                <span class="nt">&lt;description&gt;</span>password for connecting to mysql server<span class="nt">&lt;/description&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p>Replace <code class="language-plaintext highlighter-rouge">YOUR_IP</code> with the local ip address.
Replace <code class="language-plaintext highlighter-rouge">PW_FOR_HIVE</code> with the password you initiated for the hive user earlier.</p>

<h4 id="initialize-schema">Initialize Schema</h4>

<p>Now let’s make MySQL accessible from anywhere on your network.</p>

<p><code class="language-plaintext highlighter-rouge">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf</code></p>

<p>Change <code class="language-plaintext highlighter-rouge">bind-address</code> to <code class="language-plaintext highlighter-rouge">0.0.0.0</code>.</p>

<p>Restart the service for the changes take effect: <code class="language-plaintext highlighter-rouge">sudo systemctl restart mysql.service</code></p>

<p>Finally, run <code class="language-plaintext highlighter-rouge">schematool -dbType mysql -initSchema</code> to initialize the schema in the metastore database.</p>

<h4 id="start-hive-metastore">Start Hive Metastore</h4>

<p><code class="language-plaintext highlighter-rouge">hive --service metastore</code></p>

<h4 id="testing-hive">Testing Hive</h4>

<p>First start up Hive from the command line by calling <code class="language-plaintext highlighter-rouge">hive</code>.</p>

<p>Let’s create a test table:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span> <span class="n">test_table</span>
 <span class="p">(</span><span class="n">col1</span> <span class="nb">int</span> <span class="k">COMMENT</span> <span class="s1">'Integer Column'</span><span class="p">,</span>
 <span class="n">col2</span> <span class="n">string</span> <span class="k">COMMENT</span> <span class="s1">'String Column'</span><span class="p">)</span>
 <span class="k">COMMENT</span> <span class="s1">'This is test table'</span>
 <span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
 <span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">','</span>
 <span class="n">STORED</span> <span class="k">AS</span> <span class="n">TEXTFILE</span><span class="p">;</span>
</code></pre></div></div>

<p>Then insert some test data.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_table</span> <span class="k">VALUES</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="s1">'aaa'</span><span class="p">);</span>
</code></pre></div></div>

<p>Then we can view the data from the table.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">test_table</span><span class="p">;</span>
</code></pre></div></div>

<h2 id="step-6---setup-spark">Step 6 - Setup Spark</h2>

<p>Spark is a general-purpose distributed data processing engine that is suitable for use in a wide range of circumstances. On top of the Spark core data processing engine, there are libraries for SQL, machine learning, graph computation, and stream processing, which can be used together in an application. In this tutorial we’re going to setup a standalone Spark cluster using Docker and have it be able to spin up any number of workers. This reasoning behind this is we want to simulate a remote cluster and some of the configuration required for it.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">In a production setting, Spark is usually going to be configured to use Yarn and the resources already allocated for Hadoop.</span>
</div>

<p>First we need to create the Docker file. We’re going to use Spark version 2.4.4 in this tutorial but you can change it to 2.4.5 if you want the latest version and it also ships with Hadoop 2.7 to manage persistence and book keeping between nodes. In a production setting, Spark is often configured with Yarn to use the existing Hadoop environment and resources, since we only have Hadoop on one node, we’re going to run a spark standalone cluster. To configure Spark to run with Yarn requires minimal changes and you can see the difference in setup <a href="https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/">here</a>.</p>

<h4 id="setup-standalone-cluster">Setup Standalone Cluster</h4>

<p><code class="language-plaintext highlighter-rouge">nano Dockerfile</code></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Dockerfile</span>

FROM python:3.7-alpine

ARG <span class="nv">SPARK_VERSION</span><span class="o">=</span>2.4.4
ARG <span class="nv">HADOOP_VERSION</span><span class="o">=</span>2.7

RUN wget <span class="nt">-q</span> https://archive.apache.org/dist/spark/spark-<span class="k">${</span><span class="nv">SPARK_VERSION</span><span class="k">}</span>/spark-<span class="k">${</span><span class="nv">SPARK_VERSION</span><span class="k">}</span><span class="nt">-bin-hadoop</span><span class="k">${</span><span class="nv">HADOOP_VERSION</span><span class="k">}</span>.tgz <span class="se">\</span>
 <span class="o">&amp;&amp;</span> <span class="nb">tar </span>xzf spark-<span class="k">${</span><span class="nv">SPARK_VERSION</span><span class="k">}</span><span class="nt">-bin-hadoop</span><span class="k">${</span><span class="nv">HADOOP_VERSION</span><span class="k">}</span>.tgz <span class="nt">-C</span> / <span class="se">\</span>
 <span class="o">&amp;&amp;</span> <span class="nb">rm </span>spark-<span class="k">${</span><span class="nv">SPARK_VERSION</span><span class="k">}</span><span class="nt">-bin-hadoop</span><span class="k">${</span><span class="nv">HADOOP_VERSION</span><span class="k">}</span>.tgz <span class="se">\</span>
 <span class="o">&amp;&amp;</span> <span class="nb">ln</span> <span class="nt">-s</span> /spark-<span class="k">${</span><span class="nv">SPARK_VERSION</span><span class="k">}</span><span class="nt">-bin-hadoop</span><span class="k">${</span><span class="nv">HADOOP_VERSION</span><span class="k">}</span> /spark

RUN apk add shell coreutils procps
RUN apk fetch openjdk8
RUN apk add openjdk8
RUN pip3 <span class="nb">install </span>ipython

ENV PYSPARK_DRIVER_PYTHON ipython
</code></pre></div></div>

<p>Now we want to spin up a Spark master and N number of spark workers. For this we’re going to use docker-compose.</p>

<p><code class="language-plaintext highlighter-rouge">nano docker-compose.yml</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.3"</span>
<span class="na">networks</span><span class="pi">:</span>
  <span class="na">spark-network</span><span class="pi">:</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">spark-master</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span> <span class="s">.</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">spark-master</span>
    <span class="na">hostname</span><span class="pi">:</span> <span class="s">spark-master</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">&gt;</span>
      <span class="s">/bin/sh -c '</span>
      <span class="s">/spark/sbin/start-master.sh</span>
      <span class="s">&amp;&amp; tail -f /spark/logs/*'</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8080:8080</span>
      <span class="pi">-</span> <span class="s">7077:7077</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">spark-network</span>
  <span class="na">spark-worker</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span> <span class="s">.</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">spark-master</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">&gt;</span>
      <span class="s">/bin/sh -c '</span>
      <span class="s">/spark/sbin/start-slave.sh $$SPARK_MASTER</span>
      <span class="s">&amp;&amp; tail -f /spark/logs/*'</span>
    <span class="na">env_file</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">spark-worker.env</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">SPARK_MASTER=spark://spark-master:7077</span>
      <span class="pi">-</span> <span class="s">SPARK_WORKER_WEBUI_PORT=8080</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="m">8080</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">spark-network</span>
</code></pre></div></div>

<p>For the master container we’re exposing port 7077 for our applications to connect to and port 8080 for the Spark job UI. For the worker we’re connecting to our Spark master through the environment variables.</p>

<p>For more options to configure the spark worker, we add them to <code class="language-plaintext highlighter-rouge">spark-worker.env</code> file.</p>

<p><code class="language-plaintext highlighter-rouge">nano spark-worker.env</code></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">SPARK_WORKER_CORES</span><span class="o">=</span>3
<span class="nv">SPARK_WORKER_MEMORY</span><span class="o">=</span>8G
</code></pre></div></div>

<p>In this configuration, each worker will use 3 cores and have 8GB of memory. Since my machine has 6 cores, we’re going to start up 2 workers. Change these values so that it is relative to your machine. For example, if your machine only has 16GB of RAM, a good memory value might be 2 or 4GB. For a full list of environment variables and more information on stand alone mode, you can read the full documentation <a href="https://spark.apache.org/docs/latest/spark-standalone.html">here</a>. If you’re wondering about executor memory, that set when submitting or starting applications.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose build
docker-compose up <span class="nt">-d</span> <span class="nt">--scale</span> spark-worker<span class="o">=</span>2
</code></pre></div></div>

<p>Now spark is up and running and you can view the web UI at localhost:8080!</p>

<p><img src="/blog/images/bigdata_tut/spark.png" alt="" title="Spark web UI" /></p>

<h4 id="install-spark-locally">Install Spark Locally</h4>

<p>On your local machine, or any machine that’s going to be creating or using Spark, Spark needs to be installed. Since we are setting up a remote Spark cluster we have install it from the source. We’re going to use PySpark for this tutorial because I most of the time I use Python for my personal projects.</p>

<p>You can download Spark from <a href="https://archive.apache.org/dist/spark/">here</a>.</p>

<div class="Toast Toast--warning googoo" style="max-width: 1200px;">
   <span class="Toast-icon"><svg class="octicon octicon-alert octicon octicon-alert octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg></span>
   <span class="Toast-content">Ensure you download the same version you installed on your master. For this tutorial it's version 2.4.4</span>
</div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
<span class="nb">tar</span> <span class="nt">-C</span> /opt <span class="nt">-xzvf</span> spark-2.4.4-bin-hadoop2.7.tgz
</code></pre></div></div>

<p>Setup the Spark environment variables, <code class="language-plaintext highlighter-rouge">nano ~/.bashrc</code></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/opt/spark
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$SPARK_HOME</span>/bin:<span class="nv">$PATH</span>

<span class="nb">export </span><span class="nv">PYSPARK_DRIVER_PYTHON</span><span class="o">=</span><span class="s2">"jupyter"</span>
<span class="nb">export </span><span class="nv">PYSPARK_DRIVER_PYTHON_OPTS</span><span class="o">=</span><span class="s2">"notebook"</span>
<span class="nb">export </span><span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>python3
</code></pre></div></div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">If you prefer Jupyter Lab, change 'notebook', to 'lab' for PYSPARK_DRIVER_PYTHON_OPTS.</span>
</div>

<h4 id="config-files-1">Config Files</h4>

<p>To configure Spark to use our Hadoop and Hive we need to have the config files for both in the Spark config folder.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cp</span> <span class="nv">$HADOOP_HOME</span>/etc/hadoop/core-site.xml /opt/spark/conf/
<span class="nb">cp</span> <span class="nv">$HADOOP_HOME</span>/etc/hadoop/hdfs-site.xml /opt/spark/conf/
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">nano /opt/spark/conf/hive-site.xml</code></p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.metastore.uris<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>thrift://YOUR_IP:9083<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>spark.sql.warehouse.dir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hdfs://YOUR_IP:9000/user/hive/warehouse<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">hive.metastore.uris</code>: Tells Spark to interact with the Hive metastore using the Thrift API.
<code class="language-plaintext highlighter-rouge">spark.sql.warehouse.dir</code>: Tells Spark where our Hive tables are located in HDFS.</p>

<h4 id="install-pyspark">Install PySpark</h4>

<p><code class="language-plaintext highlighter-rouge">pip3 install pyspark==2.4.4</code> or replace 2.4.4 with whatever version you installed on your spark master.</p>

<p>To run PySpark connecting to our distributed cluster run:</p>

<p><code class="language-plaintext highlighter-rouge">pyspark --master spark://localhost:7077</code>, you can also replace <code class="language-plaintext highlighter-rouge">localhost</code> with your ip or a remote ip.</p>

<p>This will start up a Jupyter Notebook with the Spark Context pre defined. As a result, we now have a single environment to analyze data with or without Spark.</p>

<p>By default the executor memory is only ~1GB (1024mb). To increase it start pyspark with the following command:</p>

<p><code class="language-plaintext highlighter-rouge">pyspark --master spark://localhost:7077 --executor-memory 7g</code></p>

<p>There is a 10% overhead per executor in Spark so the most we could assign is 7200mb, but to be safe and have a nice round number we’ll go with 7.</p>

<h2 id="test-integrations">Test Integrations</h2>

<p>By default a SparkContext is automatically created and the variable is <code class="language-plaintext highlighter-rouge">sc</code>.</p>

<p>To read from our previously created hive table.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">HiveContext</span>

<span class="n">hc</span> <span class="o">=</span> <span class="n">HiveContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

<span class="n">hc</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"show tables"</span><span class="p">).</span><span class="n">show</span><span class="p">()</span>

<span class="n">hc</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"select * from test_table"</span><span class="p">).</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>To read a file from Hadoop the command would be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sparksession</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"example-pyspark-read-and-write"</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="p">(</span><span class="n">sparksession</span>
	<span class="p">.</span><span class="n">read</span>
	<span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"csv"</span><span class="p">)</span>
	<span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="s">"true"</span><span class="p">)</span>
	<span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"hdfs://YOUR_IP:9000/PATH_TO_FILE"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="practical-hadoop-use-cases">Practical Hadoop Use Cases</h2>

<p>Besides storing data, Hadoop is also utilized as a Feature Store. Let’s say you’re apart of a team or organization and they have multiple models. For each model there is a data pipeline that ingests raw data, computes and transforms the data into features. For one or two models this is perfectly fine, but what if you have multiple models? What if across those models features are being reused (i.e log normalized stock prices)?</p>

<p>Instead of each data pipeline recomputing the same features, we can create a data pipeline that computes the features once and store it in a Feature Store. The model can now pull features from the Feature Store without any redundant computation. This reduces the number of redundant computations and transformations throughout your data pipelines!</p>

<p><img src="/blog/images/bigdata_tut/featurestore.png" alt="" title="Basic Feature Store concept." /></p>

<p>Feature Stores also help with the following issues:</p>

<ul>
  <li>
    <p>Features are not reused. A common obstacle data scientists face is spending time redeveloping features instead of using previously developed features or ones developed by other teams. Feature stores allow data scientists to avoid repeat work.</p>
  </li>
  <li>
    <p>Feature definitions vary. Different teams at any one company might define and name features differently. Moreover, accessing the documentation of a specific feature (if it exists at all) is often challenging. Feature stores address this issue by keeping features and their definitions organized and consistent. The documentation of the feature store helps you create a standardized language around all of the features across the company. You know exactly how every feature is computed and what information it represents.</p>
  </li>
  <li>
    <p>There is inconsistency between training and production features. Production and research environments often use different technologies and programming languages. The data streaming in to the production system needs to be processed into features in real time and fed into a machine learning model.</p>
  </li>
</ul>

<p>If you want to take a look at a Feature Store and get started for free, I recommend <a href="https://streamsql.io/">StreamSQL</a>. StreamSQL allows you to stream your data from various sources such as HDFS, local file system, Kafka, etc. and create a data pipeline that can feed your model! It has the ability to save the feature store online or on your local HDFS for you to train your models. It also does the service of creating your test (hold out) set for you as well. They have a well documented API and is consistently improving upon it.</p>

<h2 id="feedback">Feedback</h2>

<p>I encourage all feedback about this post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help.</p>

<p>You can also reach me and follow me on Twitter at <a href="https://twitter.com/ashtonasidhu">@ashtonasidhu</a>.</p>
