{
  
    
        "post0": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://ashton-sidhu.github.io/blog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing fastlinkcheck",
            "content": ". Motivation . Recently, fastai has been hard at work improving and overhauling nbdev, a literate programming environment for python. A key feature of nbdev is automated generation of documentation from Jupyter notebooks. This documentation system adds many niceties, such as the following types of hyperlinks automatically: . Links to source code on GitHub. | Links to both internal and external documentation by introspecting variable names in backticks. | . Because documentation is so easy to create and maintain in nbdev, we find ourselves and others creating much more of it! In addition to automatic hyperlinks, we often include our own links to relevant websites, blogs and videos when documenting code. For example, one of the largest nbdev generated sites, docs.fast.ai, has more than 300 external and internal links at the time of this writing. . The Solution . Due to the continued popularity of fastai and the growth of new nbdev projects, grooming these links manually became quite tedious. We investigated solutions that could verify links for us automatically, but were not satisfied with any existing solutions. These are the features we desired: . A platform independent solution that is not tied to a specific static site generator like Jekyll or Hugo. | Intelligent introspection of external links that are actually internal links. For example, if we are building the site docs.fast.ai, a link to https://docs.fast.ai/tutorial should not result in a web request, but rather introspection of the local file system for the presence of tutorial.html in the right location. | Verification of any links to assets like CSS, data, javascript or other files. | Logs that are well organized that allow us to see each broken link or reference to a non-existent path, and the pages these are found in. | Parallelism to verify links as fast as possible. | Lightweight, easy to install with minimal dependencies. | . We tried tools such as linkchecker and pylinkvalidator, but these required your site to be first be hosted. Since we wanted to check links on a static site, hosting is overhead we wanted to avoid. . This is what led us to create fastlinkcheck, which we discuss below. . Note: For Ruby users, htmlproofer apperas to provide overlapping functionality. We have not tried this library. . A tour of fastlinkcheck . For this tour we will be referring to the files in the fastlinkcheck repo. You should clone this repo in the current directory in order to follow along: . git clone https://github.com/fastai/fastlinkcheck.git cd fastlinkcheck . Cloning into &#39;fastlinkcheck&#39;... remote: Enumerating objects: 135, done. remote: Counting objects: 100% (135/135), done. remote: Compressing objects: 100% (98/98), done. remote: Total 608 (delta 69), reused 76 (delta 34), pack-reused 473 Receiving objects: 100% (608/608), 1.12 MiB | 10.47 MiB/s, done. Resolving deltas: 100% (302/302), done. . Installation . You can install fastlinkcheck with pip: . pip install fastlinkcheck . Usage . After installing fastlinkcheck, the cli command link_check is available from the command line. We can see various options with the --help flag. . link_check --help . usage: link_check [-h] [--host HOST] [--config_file CONFIG_FILE] [--pdb] [--xtra XTRA] path Check for broken links recursively in `path`. positional arguments: path Root directory searched recursively for HTML files optional arguments: -h, --help show this help message and exit --host HOST Host and path (without protocol) of web server --config_file CONFIG_FILE Location of file with urls to ignore --pdb Run in pdb debugger (default: False) --xtra XTRA Parse for additional args (default: &#39;&#39;) . From the root of fastlinkcheck repo, We can search the directory _example/broken_links recursively for broken links like this: . link_check _example/broken_links . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Specifying the --host parameter allows you detect links that are internal by identifying links with that host name. External links are verified by making a request to the appropriate website. On the other hand, internal links are verified by inspecting the presence and content of local files. . We must be careful when using the --host argument to only pass the host (and path, if necessary) without the protocol. For example, this is how we specify the hostname if your site&#39;s url is http://fastlinkcheck.com/test.html: . link_check _example/broken_links --host fastlinkcheck.com . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . We now have one less broken link as there is indeed a file named test.html in the root of the path we are searching. However, if we add a path to the end of --host , such as fastlinkcheck.com/mysite the link would again be listed as broken because _example/broken_links/mysite/test.html does not exist: . link_check _example/broken_links --host fastlinkcheck.com/mysite . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . You can ignore links by creating a text file that contains a list of urls and paths to ignore. For example, the file _example/broken_links/linkcheck.rc contains: . cat _example/broken_links/linkcheck.rc . test.js https://www.google.com . We can use this file to ignore urls and paths with the --config_file argument. This will filter out references to the broken link /test.js from our earlier results: . link_check _example/broken_links --host fastlinkcheck.com --config_file _example/broken_links/linkcheck.rc . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Finally, if there are no broken links, link_check will not return anything. The directory _example/no_broken_links/ does not contain any HTML files with broken links: . link_check _example/no_broken_links . No broken links found! . Python . You can also use these utilities from python instead of the terminal. Please see these docs for more information. . Using link_check in GitHub Actions . The link_check CLI utility that is installed with fastlinkcheck can be very useful in continuous integration systems like GitHub Actions. Here is an example GitHub Actions workflow that uses link_check: . name: Check Links on: [workflow_dispatch, push] jobs: check-links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 - name: check for broken links run: | pip install fastlinkcheck link_check _example . We can a few more lines of code to open an issue instead when a broken link is found, using the gh cli: . ... - name: check for broken links run: | pip install fastlinkcheck link_check _example 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; [[ -s err ]] &amp;&amp; gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -R &quot;yourusername/yourrepo&quot; . We can extend this even further to only open an issue when another issue with a specific label isn&#39;t already open: . ... - name: check for broken links run: | pip install fastlinkcheck link_check &quot;docs/_site&quot; --host &quot;docs.fast.ai&quot; 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; if [[ -z $(gh issue list -l &quot;broken-link&quot;)) &amp;&amp; (-s err) ]]; then gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -l &quot;broken-link&quot; -R &quot;yourusername/yourrepo&quot; fi . See the GitHub Actions docs for more information. . Resources . The following resources are relevant for those interested in learning more about fastlinkcheck: . The fastlinkcheck GitHub repo | The fastlinkcheck docs | .",
            "url": "https://ashton-sidhu.github.io/blog/fastlinkcheck/",
            "relUrl": "/fastlinkcheck/",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
            "content": "Prefect is a workflow management system that enables users to easily take data applications and add retries, logging, dynamic mapping, caching, failure notifications, scheduling and more — all with functional Python API. Prefect allows users to take their existing code and transform it into a DAG (Directed Acyclic Graph) with dependencies already identified [1]. It simplifies the creation of ETL pipelines and dependencies and enables users to strictly focus on the application code instead of the pipeline code (looking at you Airflow). Prefect can even create distributed pipelines to parallelize your data applications. . Databricks at its core is a PaaS (Platform as a Service) that delivers fully managed Spark clusters, interactive &amp; collaborative notebooks (similar to Jupyter), a production pipeline scheduler and a platform for powering your Spark-based applications. It is integrated in both the Azure and AWS ecosystem to make working with big data simple. Databricks enables users to run their custom Spark applications on their managed Spark clusters. It even allows users to schedule their notebooks as Spark jobs. It has completely simplified big data development and the ETL process surrounding it. . Databricks has become such an integral big data ETL tool, one that I use every day at work, so I made a contribution to the Prefect project enabling users to integrate Databricks jobs with Prefect. In this tutorial we will go over just that — how you can incorporate running Databricks notebooks and Spark jobs in your Prefect flows. . Prerequisites . There is no prior knowledge needed for this post however a free Prefect account is recommended to implement the example. While this post will touch on Prefect basics, it is not an in depth Prefect tutorial. . Prefect Basics . Tasks . A task in Prefect is the equivalent of a step in your data pipeline. It is as simple as a Python function in your application or script. There are no restrictions on how simple or complex tasks can be. That being said, it’s best to follow coding best practices and develop your functions, so they only do one thing. Prefect themselves recommend this. . . In general, we encourage small tasks over monolithic ones, each task should perform a discrete logical step of your workflow, but not more. [2] By keeping tasks small, you will get the most out of Prefect’s engine such as efficient state checkpoints. . Flows . A flow is what ties all your tasks and their dependencies together. It describes dependencies between tasks, their ordering and the data flow. Flows pull together tasks and make it into a pipeline rounding out your data application. . . Native Databricks Integration in Prefect . I made a contribution to the Prefect project by the implementing the tasks DatabricksRunNow &amp; DatabricksRunSubmit enabling seamless integration between Prefect and Databricks. Through these tasks users can externally trigger a defined Databricks job or a single run of a jar, Python script or notebook. Once a task has been executed it uses Databricks native API calls to run notebooks or Spark Jobs. When the task is running it will continue to poll the current status of the run until it’s completed. Once a task is completed it will allow for downstream tasks to run if it is successful. . Creating a Flow with Databricks Tasks . Before we get started writing any code, we have to create a Prefect Secret that is going to store our Databricks connection string. From your Prefect Cloud account, click on Team from the left side menu and go to the Secrets section. This section is where you manage all the secrets for your Prefect Flows. . To generate the Databricks connection string you will need the host name of your Databricks instance as well as a PAT for your Databricks account. To create a Databricks PAT, follow these steps from the Databricks documentation. The connection string has to be a valid JSON object. The title of the secret has to be DATABRICKS_CONNECTION_STRING. . . Creating the Tasks . Let’s start our flow by defining some common tasks that we will need to run our Databricks notebooks and Spark jobs. . from prefect import task, Flow from prefect.tasks.databricks.databricks_submitjob import ( DatabricksRunNow, DatabricksSubmitRun, ) from prefect.tasks.secrets.base import PrefectSecret conn = PrefectSecret(&quot;DATABRICKS_CONNECTION_STRING&quot;) # Initialize Databricks task class as a template # We will then use the task function to pass in unique config options &amp; params RunNow = DatabricksRunNow(conn) SubmitRun = DatabricksSubmitRun(conn) . We define two task objects, RunNow and SubmitRun, to act as templates to run our Databricks jobs. We can reuse these same tasks with different configurations to easily create new Databricks jobs. Let’s create some helper tasks to dynamically create the configuration of our jobs. . @task def get_submit_config(python_params: list): &quot;&quot;&quot; SubmitRun config template for the DatabricksSubmitRun task, Spark Python Task params must be passed as a list. &quot;&quot;&quot; return { &quot;run_name&quot;: &quot;MyDatabricksJob&quot;, &quot;new_cluster&quot;: { &quot;spark_version&quot;: &quot;7.3.x-scala2.12&quot;, &quot;node_type_id&quot;: &quot;r3.xlarge&quot;, &quot;aws_attributes&quot;: { &quot;availability&quot;: &quot;ON_DEMAND&quot; }, &quot;num_workers&quot;: 10 }, &quot;spark_python_task&quot;: { &quot;python_file&quot;: &quot;/Users/ashton/databricks_task/main.py&quot;, &quot;parameters&quot;: python_params, }, } @task def get_run_now_config(notebook_params: dict): &quot;&quot;&quot; RunNow config template for the DatabricksSubmitRun task, Notebook Task params must be passed as a dictionary. &quot;&quot;&quot; return {&quot;job_id&quot;: 42, &quot;notebook_params&quot;: notebook_params} . The get_submit_config task allows us to dynamically pass parameters to a Python script that is on DBFS (Databricks File System) and return a configuration to run a single use Databricks job. You can add more flexibility by creating more parameters that map to configuration options in your Databricks job configuration. The get_run_now_config executes same task except it returns a configuration for the DatabricksRunNow task to run a preconfigured Databricks Notebooks job. The schemas of both the get_run_now_config and get_submit_config match the Run Now and Runs Submit API respectively. . . Python file parameters must be passed as a list and Notebook parameters must be passed as a dictionary. Now let’s create a flow that can run our tasks. . Creating the Flow . We’re going to create a flow that runs a preconfigured notebook job on Databricks, followed by two subsequent Python script jobs. . with Flow(&quot;Databricks-Tasks&quot;, schedule=None) as flow: run_now_config = get_run_now_config({&quot;param1&quot;: &quot;value&quot;}) submit_config_a = get_submit_config([&quot;param1&quot;]) submit_config_b = get_submit_config([&quot;param2&quot;]) run_now_task = RunNow(json=run_now_config) submit_task_a = SubmitRun(json=submit_config_a) submit_task_b = SubmitRun(json=submit_config_b) # Since Databricks tasks don&#39;t return any data dependencies we can leverage, # we have to define the dependencies between Databricks tasks themselves flow.add_edge(run_now_task, submit_task_a) flow.add_edge(submit_task_a, submit_task_b) . We first need to create the Databricks job configuration by using our get_run_now_config and get_submit_config tasks. Pass the run now configuration to the RunNow task and the submit run configuration to the SubmitRun task through the json argument. The json parameter takes in a dictionary that matches the Run Now and Submit Run APIs mentioned above. To run more Databricks jobs we instantiate either the RunNow or SubmitRun templates we created and pass in a new json job config. . One of the awesome features of a Prefect flow is that it automatically builds a DAG from your tasks. It looks at task inputs as data dependencies and from that, can infer what tasks need to be completed before other tasks can run. For example, since our run_now_task has the input run_now_config, the flow builds the DAG knowing the get_run_now_config task has to run before the run_now_task. . Some tasks don’t return data that can be used as inputs in down stream tasks. For example, the Databricks tasks only return a job ID. We can still define the inter-task dependencies of the flow by using the .add_edge function. This will add dependencies between tasks that aren’t used as inputs for further down stream tasks. For example, flow.add_edge(run_now_task, submit_task_a) says that submit_task_a is a downstream task from the run_now_task and that submit_task_a cannot run until the run_now_task has been completed. By adding the edges to the remaining Databricks task we get our final flow, which you can also view in the Prefect schematics tab. . . To the run the flow, we call the .run() method of our flow object — flow.run(). The final flow then looks like this: . from prefect import task, Flow from prefect.tasks.databricks.databricks_submitjob import ( DatabricksRunNow, DatabricksSubmitRun, ) from prefect.tasks.secrets.base import PrefectSecret @task def get_submit_config(python_params: list): &quot;&quot;&quot; SubmitRun config template for the DatabricksSubmitRun task, Spark Python Task params must be passed as a list. &quot;&quot;&quot; return { &quot;run_name&quot;: &quot;MyDatabricksJob&quot;, &quot;new_cluster&quot;: { &quot;spark_version&quot;: &quot;7.3.x-scala2.12&quot;, &quot;node_type_id&quot;: &quot;r3.xlarge&quot;, &quot;aws_attributes&quot;: { &quot;availability&quot;: &quot;ON_DEMAND&quot; }, &quot;num_workers&quot;: 10 }, &quot;spark_python_task&quot;: { &quot;python_file&quot;: &quot;/Users/ashton/databricks_task/main.py&quot;, &quot;parameters&quot;: python_params, }, } @task def get_run_now_config(notebook_params: dict): &quot;&quot;&quot; RunNow config template for the DatabricksSubmitRun task, Notebook Task params must be passed as a dictionary. &quot;&quot;&quot; return {&quot;job_id&quot;: 42, &quot;notebook_params&quot;: notebook_params} conn = PrefectSecret(&quot;DATABRICKS_CONNECTION_STRING&quot;) # Initialize Databricks task class as a template # We will then use the task function to pass in unique config options &amp; params RunNow = DatabricksRunNow(conn) SubmitRun = DatabricksSubmitRun(conn) with Flow(&quot;Databricks-Tasks&quot;, schedule=None) as flow: run_now_config = get_run_now_config({&quot;param1&quot;: &quot;value&quot;}) submit_config_a = get_submit_config([&quot;param1&quot;]) submit_config_b = get_submit_config([&quot;param2&quot;]) run_now_task = RunNow(json=run_now_config) submit_task_a = SubmitRun(json=submit_config_a) submit_task_b = SubmitRun(json=submit_config_b) # Since Databricks tasks don&#39;t return any data dependencies we can leverage, # we have to define the dependencies between Databricks tasks themselves flow.add_edge(run_now_task, submit_task_a) flow.add_edge(submit_task_a, submit_task_b) flow.run() # flow.register(&quot;YOUR_PROJECT&quot;) to register your flow on the UI . Conclusion . You now have all the knowledge you need to run Databricks Notebooks and Spark jobs as part of your ETL flows. For more information on Prefect and Databricks jobs, I recommend reading their documentation found here and here. . Feedback . As always, I encourage any feedback about my post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help. . You can also reach me and follow me on Twitter at @ashtonasidhu. . References . https://docs.prefect.io/core/, Prefect Documentation . | https://docs.prefect.io/core/getting_started/first-steps.html, Prefect Getting Started . |",
            "url": "https://ashton-sidhu.github.io/blog/prefect/databricks/etl/tutorial/guide/spark/2020/11/02/prefect-databricks.html",
            "relUrl": "/prefect/databricks/etl/tutorial/guide/spark/2020/11/02/prefect-databricks.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Tutorial: Integrating Python with Compiler Languages",
            "content": "Python - a programming language that is human readable, easy to learn, and most importantly, easy to use. It is no wonder that it is one of the most popular languages today. From being able to build web applications, create automation scripts, analyze data and build machine learning models, Python is a jack of all trades. As with anything it has its short comings which include its speed and lack of granular access to machine hardware. This is due to Python being an interpreter-based language and not a compiler-based language. . Luckily for us, there is a way to solve that problem. Python has a native library that allows you to use functions built in compiled languages. This gives you the ability to leverage both the powers of Python and compiler based languages. Some of the most popular data science libraries such as Pandas and XGBoost have modules written in C/C++ (compiler-based languages) to overcome Python’s speed and performance issues while still having a user friendly Python API. . In this post, we are going to walk through interpreter based languages vs. compiler based languages, Python’s built in ctypes module and an example of how to use modules built from a compiled language. From there you will be able to integrate Python with other languages in your data science or machine learning projects. . Interpreters vs. Compilers . As I mentioned above, Python is an interpreted language. In order to execute code on a machine, the code first has to get translated to “machine code”. . . Machine code is just binary or hexadecimal instructions. The main difference between interpreted and compiled languages is that interpreted languages get executed line by line and passed through a translator that converts each line to machine code. . Compilers require a build step that translates all the code at once into an application (or binary) that can be executed. During the build phase there is a compiler that will optimize the translation from written code to machine code. The compiler’s optimizations are one of compiled based-languages are faster than interpreter-based languages. Common compiled languages are C, C++ and Go. . . This a high level comparison between interpreters and compilers. More in-depth knowledge would require individual research or a separate topic entirely. C Types . Python has built in libraries to be able to call modules built in compiled languages. It gives developers the ultimate flexibility to use compiled languages for tasks that Python isn’t well equipped to handle - all the while still building the core of the application in Python. . The main library that we will be using to interact with modules from a compiled application is ctypes. It provides C compatible data types and calling functions from applications built in compiled languages. It gives you the ability to wrap these languages in pure Python. To be able to do this, Python first has to convert its function argument types into C native types. This allows it to be compatible with compiled language function’s argument types. The figure below explains the process at a high level when interacting with functions from compiled languages. . . The example in the next section will be using Go so the image is Go specific, but the principle applies to the other compiled languages. . Walk Through . Below is a Go function I wrote that gets the closing price of a stock using the Alpha Vantage API. I’ll go over the key lines that allow us to create a Python wrapper for it. . package main import &quot;C&quot; import ( &quot;encoding/json&quot; &quot;fmt&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;net/http&quot; &quot;os&quot; &quot;time&quot; ) // APIKEY ... Alpha Vantage API key, stored as env variable var APIKEY string = os.Getenv(&quot;ALPHA_API_KEY&quot;) // ENDPOINT ... Alpha Vantage API daily stock data endpoint var ENDPOINT string = &quot;https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&quot; // Data ... Data structure type Data struct { MetaData MetaData `json:&quot;Meta Data&quot;` TimeSeries map[string]interface{} `json:&quot;Time Series (Daily)&quot;` } // MetaData ... Stock metadata structure type MetaData struct { Info string `json:&quot;1. Information&quot;` Symbol string `json:&quot;2. Symbol&quot;` LastRefresh string `json:&quot;3. Last Refreshed&quot;` OutputSize string `json:&quot;4. Output Size&quot;` TimeZone string `json:&quot;5. Time Zone&quot;` } // FinData ... Daily Financial Data json structure type FinData struct { Open string `json:&quot;1. open&quot;` High string `json:&quot;2. high&quot;` Low string `json:&quot;3. low&quot;` Close string `json:&quot;4. close&quot;` AdjClose string `json:&quot;5. adjusted close&quot;` Volume string `json:&quot;6. volume&quot;` DivAmount string `json:&quot;7. dividend amount&quot;` SplitCoeff string `json:&quot;8. split coefficient&quot;` } func main() {} //export getPrice func getPrice(ticker *C.char, date *C.char) *C.char { tickerDate := C.GoString(date) stock := C.GoString(ticker) query := fmt.Sprintf(&quot;%s&amp;symbol=%s&amp;apikey=%s&quot;, ENDPOINT, stock, APIKEY) client := http.Client{ Timeout: time.Second * 10, // Timeout after 5 seconds } req, err := http.NewRequest(http.MethodGet, query, nil) if err != nil { log.Fatal(err) } req.Header.Set(&quot;User-Agent&quot;, &quot;stock-api-project&quot;) resp, getErr := client.Do(req) if getErr != nil { log.Fatal(getErr) } defer resp.Body.Close() respBody, _ := ioutil.ReadAll(resp.Body) dailyData := Data{} json.Unmarshal(respBody, &amp;dailyData) // Encode Interface as bytes dailyFinDataMap := dailyData.TimeSeries[tickerDate] dfdByte, _ := json.Marshal(dailyFinDataMap) // Map interface to FinData struct dailyFinData := FinData{} json.Unmarshal(dfdByte, &amp;dailyFinData) return C.CString(dailyFinData.AdjClose) } . import “C” - Import the C package (aka cgo) to have access to C data types. . func main() {} - An empty main function to ensure we still have an executable. . //export getPrice - Export the function so that we can expose it to be accessed by Python. . func getPrice(ticker *C.char, date *C.char) *C.char { - The function arguments need to be C types. . tickerDate := C.GoString(date) - Convert the function arguments to their native Go types. . return C.CString(dailyFinData.AdjClose) - The return value has to be a native C type. . To be able to use this in Python we have to compile this program, go build -o stock-api.so -buildmode=c-shared main.go . Below is the associating Python wrapper that calls our exported Go function getPrice. . from ctypes import * def get_price(ticker: str) -&gt; Dict[str, str]: &quot;&quot;&quot;Gets the adjusted closing price of all stocks.&quot;&quot;&quot; lib = cdll.LoadLibrary(&quot;./stock-api.so&quot;) lib.getPrice.argtypes = [c_char_p, c_char_p] lib.getPrice.restype = c_char_p curr_date = str(date.today()) price = lib.getPrice(ticker.encode(), curr_date.encode()).decode() return price . from ctypes import * - Import everything from ctypes as per the documentation. . lib = cdll.LoadLibrary(&quot;./stock-api.so&quot;) - Load in the binary or compiled application. . lib.getPrice.argtypes = [c_char_p, c_char_p] - Set the getPrice function argument types to the corresponding C types. . lib.getPrice.restype = c_char_p - Set the return type to the corresponding C type. . price = lib.getPrice(ticker.encode(), curr_date.encode()).decode() - We call the getPrice function and convert the Python string arguments into bytes that can be passed to the Go function by calling .encode. We then receive the output from Go function as bytes so we decode it to convert it to a Python string. . Conclusion . You now have most of the knowledge you need to get started creating wrappers for compiled language functions. No longer are you bound by the cons of interpreter based languages. You can create modules in languages that better suit your use case, and then fall back on Python to build out a user friendly API or the core of your application. Happy coding! . Feedback . I encourage any and all feedback about any of my posts and tutorials. You can message me on twitter or e-mail me at sidhuashton@gmail.com . .",
            "url": "https://ashton-sidhu.github.io/blog/python/data%20science/go/machine%20learning/2020/10/25/ctypes-go.html",
            "relUrl": "/python/data%20science/go/machine%20learning/2020/10/25/ctypes-go.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Data Scientist's Guide to Hacktoberfest",
            "content": "What is Hacktoberfest . . Open source projects are a pillar for the data industry, without them projects such as Pandas, Numpy Sci-kit Learn and the entire big data Apache stack would not exist. Hacktoberfest is a month long event where developers can contribute to open source projects on Github and receive free swag. To be eligible to receive the free swag, you must commit four successful Pull Requests on projects that have opted in to Hacktoberfest. To try and easily get the free swag, some contributors submit non meaningful contributions, leading to unhappy project maintainers. . In this post we’ll go through how you can give back to the community and make a meaningful contributions open source projects. . How I got into Open Source . . I first got into Open Source when I was working with two projects, interpret and pandas-summary. In both cases the usage of the projects had not completely covered my use cases, albeit they were very minor. . With interpret I was trying to use the library to explain/interpret a Sklearn classifier. The underlying issue is that the .predict method returns an integer (0, 1, etc.) for classifiers. When these integer values were used to calculate the Morris Sensitivity, an error was thrown saying that the data had to be a float. In this case, the fix was simple - cast the integers returned as integers by the Sklearn classifier to a float! Before the fix, anyone who tried to interpret a Sklearn classifier would get an error and this simple contribution allowed all users to interpret Sklearn classification models. . With pandas-summary, I was using this library to automate descriptive and statistical analysis of column data. The only issue was that every time I used the library, a histogram was always plotted, which was a nuisance. As a quality of life fix, I added a simple flag that users could specify if they want to plot the histogram. . My first two contributions were nothing special, and were quite simple, but from these two contributions I learned the basics of how to make a Pull Request to a remote project, follow contribution guidelines as well as interact &amp; communicate with project maintainers via Github, Slack, etc. . From there I went on to be a major contributor of the pandas-bokeh library, contributed bug fixes and UI improvements to Nteract and added major feature integrations to Prefect. . Tips . . Learn how to make a Pull Request . A pull request is a method of contributing your code into a project. To start off, find a project you want to contribute to and fork the project by clicking the fork button at the top right corner of the project page. This will create a copy of the project in its current state under your repositories. . . Once you have forked the project, navigate to the repository under your Projects in Github. Under the project name it should say forked from .... . Clone the forked project to your local computer. . Create a new branch for you change or bug fix: git checkout -b your_branch_name. . Make your changes, commit them following the Contribution Guidelines of the project. . Push your changes to your forked project repository: git push -u origin your_branch_name. . Navigate to the forked repository project page. You will see a prompt to create a pull request. If you don’t navigate to the pull requests tab and create one from there by selecting New Pull Request. . . Fill out the pull request template, if there is one, or one that is outlined in the Contributing guidelines. Once that is completed, your pull request is good to go and wait for feedback! . Start with a library you use often, big or small . The first step towards your first open source contribution is to pick a project you want to contribute to. The best way that I have experienced is to contribute to projects you use often or are actively using. If you have extensively used a project, you have probably come across some across some bugs or enhancements that will improve the quality of the project. This is how I started my journey in Open Source, by trying to improve projects that I was using daily. . Look at existing Github Issues . If you are looking for an issue or way to contribute to a project, a good place to start is Github’s built in Issues tab. This is where users and project maintainers can log bugs and feature enhancements. Project maintainers will go through these issues and tag them, gather more information, add meta data, etc. One of the tags they will add is a “good first issue” tag to inform potential contributors that this issue is good for first time contributors or contributors who are new to the project. . . These issues are recommended for contributors who are either new to open source or to the project itself to help them get started and make a contribution. Leverage these if you can’t find your own bug to fix or enhancement to add. . Read the Contribution Guidelines . There is nothing worse than putting in all this work of finding a project, isolating a bug or developing a new feature and to have it rejected or not even looked at because you didn’t follow coding, formatting or commit message standards. The good news is that all of these are available, usually in a Contribution section in the projects README or in the Contributing Guidelines section of the project. Projects will often have automated formatting checks that run when you create a pull request and your pull request won’t often be looked at until these basic checks are passed. . . If you don&#39;t see a contribution section or a project doesn&#39;t have Contributing Guidelines, don&#39;t just do whatever you want.&emsp;1) Follow the coding &amp; documentation style in the project as close as you can.&emsp;2) Follow coding &amp; documentation best practices. Every Contribution Matters . Every contribution in open source matters regardless how big or small. Whether it’s from a usability perspective or reading the documentation, if you are experiencing a bug or a grievance with a project there are others who are experiencing it as well. Documentation is a large component of open source software as every project needs it. It is often a great place to start contributing to a project as you gain an understanding of what a project is about. It provides background information of design decisions and considerations of the project, which will in turn help you in understand the code. . Documentation is the first place users go to find information and the more thoroughly documented a project is, the more of a user base it will have. Developers love documented projects and when a project doesn’t have documentation, or has poor documentation, a developer will think twice before adding it to their work flow. Adding documentation or fixing even the smallest bug may impact hundreds or even thousands of users who use that project every day and many will thank you for it. . What Happens Next . . Very rarely will your contribution get merged right away. Within a couple of days someone from the project team will comment their feedback or notify you that they are reviewing your pull request. Address the comments, ask questions, clarify anything you do not understand, make the proposed changes, if there are any, and your change with get merged shortly after! . If you do not receive any feedback on your pull request within a week, message a project maintainer and politely ask them what the status is. On larger projects there are often tons of pull requests and they may have forgotten about the pull request or have not got around to reviewing it yet. . If at this point you have not received a reply, which does not happen often (has never happened to me), take the skills and learning points from this project and move on to the next project. Once you make the pull request and have messaged the project maintainers, the rest is out of your hands. This is the only real unfortunate part of open source and one you should not take to heart. . Benefits . . Become a better Engineer or Scientist . Whether you are a Data Engineer, ML Engineer or Data Scientist contributing to Open Source will help you become better and progress in your field. From understanding how projects are built and structured, gaining deep intricate knowledge of a key library, navigating large code bases, writing production level code or even just learning a new method to solve a problem. All of these skills will translate directly into your profession or your next project. . Meet new Engineers, Developers &amp; Scientists . The greatest benefit of contributing to Open Source is the opportunity to work and interact with the minds who created a tool that is used by thousands of people world wide. You get direct insight into how they they created a solution to solve a widespread problem. Furthermore, you may end up connecting, bounce ideas off one another and collaborate on future projects together. Personally, I’ve connected with project maintainers whose project I have contributed to and have kept in touch with them on Twitter and LinkedIn. . Conclusion . . Today you may not be able to contribute a new feature, but being around the project, reading the code, reading the documentation, all of it gives you insight into the project. From there, a small contribution to documentation, might lead to a bug fix that was documented, which leads to a higher understanding of the project, which then leads to your first feature. . The moral of the story is, contribute in any way you can and eventually you will reach that goal of developing the new feature, starting your own OS project and even becoming a key contributor to a project. . Feedback . . I encourage any and all feedback about any of my posts. You can message me on twitter or e-mail me at sidhuashton@gmail.com. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/data%20science/open%20source/hacktoberfest/ml/2020/10/08/hacktoberfest-help.html",
            "relUrl": "/markdown/data%20science/open%20source/hacktoberfest/ml/2020/10/08/hacktoberfest-help.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
            "content": "Jupyter Notebook provides a great platform to produce human-readable documents containing code, equations, analysis, and their descriptions. Some even consider it a powerful development when combining it with NBDev. For such an integral tool, the out of the box start up is not the best. Each use requires starting the Jupyter web application from the command line and entering your token or password. The entire web application relies on that terminal window being open. Some might “daemonize” the process and then use nohup to detach it from their terminal, but that’s not the most elegant and maintainable solution. . Lucky for us, Jupyter has already come up with a solution to this problem by coming out with an extension of Jupyter Notebooks that runs as a sustainable web application and has built-in user authentication. To add a cherry on top, it can be managed and sustained through Docker allowing for isolated development environments. . By the end of this post we will leverage the power of JupyterHub to access a Jupyter Notebook instance which can be accessed without a terminal, from multiple devices within your network, and a more user friendly authentication method. . Prerequisites . . A basic knowledge of Docker and the command line would be beneficial in setting this up. . I recommend doing this on the most powerful device you have and one that is turned on for most of the day, preferably all day. One of the benefits of this setup is that you will be able to use Jupyter Notebook from any device on your network, but have all the computation happen on the device we configure. . What is Jupyter Hub . . JupyterHub brings the power of notebooks to groups of users. The idea behind JupyterHub was to scale out the use of Jupyter Notebooks to enterprises, classrooms, and large groups of users. Jupyter Notebook, however, is supposed to run as a local instance, on a single node, by a single developer. Unfortunately, there was no middle ground to have the usability and scalability of JupyterHub and the simplicity of running a local Jupyter Notebook. That is, until now. . JupyterHub has pre-built Docker images that we can utilize to spawn a single notebook on a whim, with little to no overhead in technical complexity. We are going to use the combination of Docker and JupyterHub to access Jupyter Notebooks from anytime, anywhere, at the same URL. . Architecture . . The architecture of our JupyterHub server will consist of 2 services: JupyterHub and JupyterLab. JupyterHub will be the entry point and will spawn JupyterLab instances for any user. Each of these services will exist as a Docker container on the host. . . Building the Docker Images . . To build our at-home JupyterHub server we will use the pre-built Docker images of JupyterHub &amp; JupyterLab. . Dockerfiles . The JupyterHub Docker image is simple. . FROM jupyterhub/jupyterhub:1.2 # Copy the JupyterHub configuration in the container COPY jupyterhub_config.py . # Download script to automatically stop idle single-user servers COPY cull_idle_servers.py . # Install dependencies (for advanced authentication and spawning) RUN pip install dockerspawner . We use the pre-built JupyterHub Docker Image and add our own configuration file to stop idle servers, cull_idle_servers.py. Lastly, we install additional packages to spawn JupyterLab instances via Docker. . Docker Compose . To bring everything together, let’s create a docker-compose.yml file to define our deployments and configuration. . version: &#39;3&#39; services: # Configuration for Hub+Proxy jupyterhub: build: . # Build the container from this folder. container_name: jupyterhub_hub # The service will use this container name. volumes: # Give access to Docker socket. - /var/run/docker.sock:/var/run/docker.sock - jupyterhub_data:/srv/jupyterlab environment: # Env variables passed to the Hub process. DOCKER_JUPYTER_IMAGE: jupyter/tensorflow-notebook DOCKER_NETWORK_NAME: ${COMPOSE_PROJECT_NAME}_default HUB_IP: jupyterhub_hub ports: - 8000:8000 restart: unless-stopped # Configuration for the single-user servers jupyterlab: image: jupyter/tensorflow-notebook command: echo volumes: jupyterhub_data: . The key environment variables to note are DOCKER_JUPYTER_IMAGE and DOCKER_NETWORK_NAME. JupyterHub will create Jupyter Notebooks with the images defined in the environment variable.For more information on selecting Jupyter images you can visit the following Jupyter documentation. . DOCKER_NETWORK_NAME is the name of the Docker network used by the services. This network gets an automatic name from Docker Compose, but the Hub needs to know this name to connect the Jupyter Notebook servers to it. To control the network name we use a little hack: we pass an environment variable COMPOSE_PROJECT_NAME to Docker Compose, and the network name is obtained by appending _default to it. . Create a file called .env in the same directory as the docker-compose.yml file and add the following contents: . COMPOSE_PROJECT_NAME=jupyter_hub . Stopping Idle Servers . Since this is our home setup, we want to be able to stop idle instances to preserve memory on our machine. JupyterHub has services that can run along side it and one of them being jupyterhub-idle-culler. This service stops any instances that are idle for a prolonged duration. . To add this servive, create a new file called cull_idle_servers.py and copy the contents of jupyterhub-idle-culler project into it. . . Ensure `cull_idle_servers.py` is in the same folder as the Dockerfile. To find out more about JupyterHub services, check out their official documentation on them. . Jupyterhub Config . To finish off, we need to define configuration options such, volume mounts, Docker images, services, authentication, etc. for our JupyterHub instance. . Below is a simple jupyterhub_config.py configuration file I use. . import os import sys c.JupyterHub.spawner_class = &#39;dockerspawner.DockerSpawner&#39; c.DockerSpawner.image = os.environ[&#39;DOCKER_JUPYTER_IMAGE&#39;] c.DockerSpawner.network_name = os.environ[&#39;DOCKER_NETWORK_NAME&#39;] c.JupyterHub.hub_connect_ip = os.environ[&#39;HUB_IP&#39;] c.JupyterHub.hub_ip = &quot;0.0.0.0&quot; # Makes it accessible from anywhere on your network c.JupyterHub.admin_access = True c.JupyterHub.services = [ { &#39;name&#39;: &#39;cull_idle&#39;, &#39;admin&#39;: True, &#39;command&#39;: [sys.executable, &#39;cull_idle_servers.py&#39;, &#39;--timeout=42000&#39;] }, ] c.Spawner.default_url = &#39;/lab&#39; notebook_dir = os.environ.get(&#39;DOCKER_NOTEBOOK_DIR&#39;) or &#39;/home/jovyan/work&#39; c.DockerSpawner.notebook_dir = notebook_dir c.DockerSpawner.volumes = { &#39;/home/sidhu&#39;: &#39;/home/jovyan/work&#39; } . Take note of the following configuration options: . &#39;command&#39;: [sys.executable, &#39;cull_idle_servers.py&#39;, &#39;--timeout=42000&#39;] : Timeout is the number of seconds until an idle Jupyter instance is shut down. . | c.Spawner.default_url = &#39;/lab&#39;: Uses Jupyterlab instead of Jupyter Notebook. Comment out this line to use Jupyter Notebook. . | &#39;/home/sidhu&#39;: &#39;/home/jovyan/work&#39;: I mounted my home directory to the JupyterLab home directory to have access to any projects and notebooks I have on my Desktop. This also allows us to achieve persistence in the case we create new notebooks, they are saved to our local machine and will not get deleted if our Jupyter Notebook Docker container is deleted. . | . Remove this line if you do not wish to mount your home directory and do not forget to change sidhu to your user name. . Start the Server . . To start the server, simply run docker-compose up -d, navigate to localhost:8000 in your browser and you should be able to see the JupyterHub landing page. . . To access it on other devices on your network such asva laptop, an iPad, etc, identify the IP of the host machine by running ifconfig on Unix machines &amp; ipconfig on Windows. . . From your other device, navigate to the IP you found on port 8000: http://IP:8000 and you should see the JupyterHub landing page! . Authenticating . That leaves us with the last task of authenticating to the server. Since we did not set up a LDAP server or OAuth, JupyterHub will use PAM (Pluggable Authentication Module) authentication to authenticate users. This means JupyterHub uses the user name and passwords of the host machine to authenticate. . To make use of this, we will have to create a user on the JupyterHub Docker container. There are other ways of doing this such as having a script placed on the container and executed at container start up but we will do it manually as an exercise. If you tear down or rebuild the container you will have to recreate users. . . I do not recommend hard coding user credentials into any script or Dockerfile. 1) Find the JupyterLab container ID: docker ps -a . . 2) “SSH” into the container: docker exec -it $YOUR_CONTAINER_ID bash . 3) Create a user and follow the terminal prompts to create a password: useradd $YOUR_USERNAME . 4) Sign in with the credentials and you’re all set! . . You now have a ready to go Jupyter Notebook server that can be accessed from any device, in the palm of your hands! Happy Coding! . Feedback . . I encourage any and all feedback about any of my posts and tutorials. You can message me on twitter or e-mail me at sidhuashton@gmail.com. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/data%20science/jupyter/ml/2020/10/03/jupyterhub.html",
            "relUrl": "/markdown/data%20science/jupyter/ml/2020/10/03/jupyterhub.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c=3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c=3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c=3, d=4): pass @delegates(basefoo, but=[&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://ashton-sidhu.github.io/blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "What's New in Aethos 2.0",
            "content": "In late 2019 I released Aethos 1.0, the first iteration of a package to automate common data science techniques. Since then I’ve received great feedback on how to improve Aethos which I’ll introduce here! It will be a lot of code examples to show the power and versatility of the package. . You can view the previous posts about Aethos on my blog! . Intro to Aethos . Modelling with Aethos . What is Aethos? . For those new to Aethos, Aethos is a Python library of automated data science techniques and use cases from missing value imputation, NLP pre-processing, feature engineering, data visualization to modelling, model analysis and model deployment. . To see the full capabilities and the rest of the techniques and models you can run, checkout the project page on Github! . Problems with Aethos 1.0 . Alot of the problems with the first version of Aethos were related to the usability of the package and its API. The major problems were: . Slow import times due to the number of files and coupled packages. . | Having 2 objects for end to end analysis - Data for transformations and Model for modelling . | Model object had every model and was not specific to Supervised or Unsupervised problems. . | Unintuitive API calls for adding new columns to the underlying DataFrames . | Reporting feature was, well, garbage and becoming redundant with external tools like converting notebooks to pdfs. . | API had limited use cases. You couldn&#39;t just analyze your data, or just analyze a model you trained without Aethos. . | Aethos and Pandas were not interchangeable and did not work together when transforming data. . | . What&#39;s new in Aethos 2.0 . Aethos 2.0 looks to address the intuitiveness and usability of the package to make it easier to use and understand. It also addresses the ability to work with Pandas Dataframes side by side with Aethos. . Reduced import time of the package by simplifying and decoupling of the Aethos modules. . | Only 1 object to analyze, visualize, transform, model and analyze results. . | Can now specify the type of problem - Classification, Regression or Unsupervised and only see the models specific to those problems. . | Removed the complexity of adding data to the underlying dataframes through Aethos objects. You can access the underlying dataframes with the x_train and x_test properties. . | Removed reporting feature. . | Introduced new objects to support new cases: . Analysis: To analyze, visualize and run statistical analysis (t-test, anova, etc.) on your data. . | Classification: To analyze, visualize, run statistical analysis, transform and impute your data to run classification models. . | Regression: To analyze, visualize, run statistical analysis, transform and impute your data to run regression models. . | Unsupervised: To analyze, visualize, run statistical analysis, transform and impute your data to run unsupervised models. . | ClassificationModelAnalysis: Interpret, analyze and visualize classification model results. . | RegressionModelAnalysis: Interpret, analyze and visualize regression model results. . | UnsupervisedModelAnalysis: Interpret, analyze and visualize unsupervised model results. . | TextModelAnalysis: Interpret, analyze and visualize text model results. . | . | Removed dot notation when accessing DataFrame columns. . | Can now chain methods together. . | . . Note: The model analysis objects get automatically initialized when you run a model with Aethos. They can also be initialized by themselves by supplying a model object, train data and test data. . Examples . !pip install aethos . import pandas as pd import aethos as at at.options.track_experiments = True # Enable experiment tracking with MLFlow . To showcase each of the objects let&#39;s load in the titanic dataset. . orig_data = pd.read_csv(&#39;https://raw.githubusercontent.com/Ashton-Sidhu/aethos/develop/examples/data/train.csv&#39;) . orig_data.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . Analysis . The analysis objects is mainly for quick, easy analysis and visualization of data. It doesn&#39;t have the ability to run automated cleaning and transformation techniques of Aethos, just visualizations and statistical tests. It also does not split your data, but you do have the option to provide a test set. . df = at.Analysis(orig_data, target=&#39;Survived&#39;) . df.describe() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . count 891 | 891 | 891 | NaN | NaN | 714 | 891 | 891 | NaN | 891 | NaN | NaN | . mean 446 | 0.383838 | 2.30864 | NaN | NaN | 29.6991 | 0.523008 | 0.381594 | NaN | 32.2042 | NaN | NaN | . std 257.354 | 0.486592 | 0.836071 | NaN | NaN | 14.5265 | 1.10274 | 0.806057 | NaN | 49.6934 | NaN | NaN | . min 1 | 0 | 1 | NaN | NaN | 0.42 | 0 | 0 | NaN | 0 | NaN | NaN | . 25% 223.5 | 0 | 2 | NaN | NaN | 20.125 | 0 | 0 | NaN | 7.9104 | NaN | NaN | . 50% 446 | 0 | 3 | NaN | NaN | 28 | 0 | 0 | NaN | 14.4542 | NaN | NaN | . 75% 668.5 | 1 | 3 | NaN | NaN | 38 | 1 | 0 | NaN | 31 | NaN | NaN | . max 891 | 1 | 3 | NaN | NaN | 80 | 8 | 6 | NaN | 512.329 | NaN | NaN | . counts 891 | 891 | 891 | 891 | 891 | 714 | 891 | 891 | 891 | 891 | 204 | 889 | . uniques 891 | 2 | 3 | 891 | 2 | 88 | 7 | 7 | 681 | 248 | 147 | 3 | . missing 0 | 0 | 0 | 0 | 0 | 177 | 0 | 0 | 0 | 0 | 687 | 2 | . missing_perc 0% | 0% | 0% | 0% | 0% | 19.87% | 0% | 0% | 0% | 0% | 77.10% | 0.22% | . types numeric | bool | numeric | unique | bool | numeric | numeric | numeric | categorical | numeric | categorical | categorical | . df.missing_values . Train set missing values. Total Percent . Cabin 687 | 77.10% | . Age 177 | 19.87% | . Embarked 2 | 0.22% | . Fare 0 | 0.00% | . Ticket 0 | 0.00% | . Parch 0 | 0.00% | . SibSp 0 | 0.00% | . Sex 0 | 0.00% | . Name 0 | 0.00% | . Pclass 0 | 0.00% | . Survived 0 | 0.00% | . PassengerId 0 | 0.00% | . | . df.column_info() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . counts 891 | 891 | 891 | 891 | 891 | 714 | 891 | 891 | 891 | 891 | 204 | 889 | . uniques 891 | 2 | 3 | 891 | 2 | 88 | 7 | 7 | 681 | 248 | 147 | 3 | . missing 0 | 0 | 0 | 0 | 0 | 177 | 0 | 0 | 0 | 0 | 687 | 2 | . missing_perc 0% | 0% | 0% | 0% | 0% | 19.87% | 0% | 0% | 0% | 0% | 77.10% | 0.22% | . types numeric | bool | numeric | unique | bool | numeric | numeric | numeric | categorical | numeric | categorical | categorical | . df.standardize_column_names() . passengerid pclass name sex age sibsp parch ticket fare cabin embarked survived . 0 1 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 0 | . 1 2 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 1 | . 2 3 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 1 | . 3 4 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 1 | . 4 5 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 0 | . df.describe_column(&#39;fare&#39;) . mean 32.2042 std 49.6934 variance 2469.44 min 0 max 512.329 mode 8.05 5% 7.225 25% 7.9104 50% 14.4542 75% 31 95% 112.079 iqr 23.0896 kurtosis 33.3981 skewness 4.78732 sum 28693.9 mad 28.1637 cv 1.54307 zeros_num 15 zeros_perc 1.68% deviating_of_mean 20 deviating_of_mean_perc 2.24% deviating_of_median 53 deviating_of_median_perc 5.95% top_correlations counts 891 uniques 248 missing 0 missing_perc 0% types numeric Name: fare, dtype: object . df.data_report() . Summarize dataset: 100%|██████████| 26/26 [00:04&lt;00:00, 6.31it/s, Completed] Generate report structure: 100%|██████████| 1/1 [00:01&lt;00:00, 1.94s/it] Render HTML: 100%|██████████| 1/1 [00:00&lt;00:00, 1.07it/s] . . Easily view the histogram of multiple features. . df.histogram(&#39;age&#39;, &#39;fare&#39;, hue=&#39;survived&#39;) . Create a configurable correlation matrix. . df.correlation_matrix(data_labels=True, hide_mirror=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd038322748&gt; . We can easily plot the average price each age paid for a ticket. . df.barplot(x=&#39;age&#39;, y=&#39;fare&#39;, method=&#39;mean&#39;, labels={&#39;age&#39;: &#39;Age&#39;, &#39;fare&#39;: &#39;Fare&#39;}, asc=False) . . . We can also easily view the relationship between age and fair and see the difference between those who survived and who didn&#39;t. . df.scatterplot(x=&#39;age&#39;, y=&#39;fare&#39;, color=&#39;survived&#39;, labels={&#39;age&#39;: &#39;Age&#39;, &#39;fare&#39;: &#39;Fare&#39;}, marginal_x=&#39;histogram&#39;, marginal_y=&#39;histogram&#39;) . . . You can visualize other plots like raincloud, violin, box, pairwise, etc. I recommend checking out the examples for more! . One of the big changes is that ability to work with pandas side by side. If you want to transform and work with data solely with Pandas, the Analysis object will reflect those changes. This allows you to use Aethos solely for automated analysis and Pandas for transformations. . To demonstrate this we will make a new boolean feature to see if a passenger was a child using the original pandas dataframe we created . orig_data[&#39;is_child&#39;] = (orig_data[&#39;age&#39;] &lt; 18).astype(int) orig_data.head() . passengerid survived pclass name sex age sibsp parch ticket fare cabin embarked is_child . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 0 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 0 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 0 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 0 | . Now let&#39;s see it in our Analysis object. . df.head() . passengerid survived pclass name sex age sibsp parch ticket fare cabin embarked is_child . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 0 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 0 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 0 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 0 | . df.boxplot(x=&#39;is_child&#39;, y=&#39;fare&#39;, color=&#39;survived&#39;) . . . You can still run pandas functions on Aethos objects. . df.nunique() . passengerid 891 survived 2 pclass 3 name 891 sex 2 age 88 sibsp 7 parch 7 ticket 681 fare 248 cabin 147 embarked 3 is_child 2 dtype: int64 . df[&#39;age&#39;].nunique() . 88 . New Features . Introduced in Aethos 2.0 are some new analytic techniques. . Predictive Power Score . The predictive power score is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). Credits go to 8080Labs for creating this library and you can get more info here . df.predictive_power(data_labels=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd0384a2198&gt; . AutoViz . AutoViz auto visualizes your data and displays key plots based off the characteristics of your data. Credits go to AutoViML for creating this library and you can get more info here. . df.autoviz() . Imported AutoViz_Class version: 0.0.68. Call using: from autoviz.AutoViz_Class import AutoViz_Class AV = AutoViz_Class() AutoViz(filename, sep=&#39;,&#39;, depVar=&#39;&#39;, dfte=None, header=0, verbose=0, lowess=False,chart_format=&#39;svg&#39;,max_rows_analyzed=150000,max_cols_analyzed=30) To remove previous versions, perform &#39;pip uninstall autoviz&#39; Shape of your Data Set: (891, 13) Classifying variables in data set... 12 Predictors classified... This does not include the Target column(s) 4 variables removed since they were ID or low-information variables Total Number of Scatter Plots = 3 Nothing to add Plot not being added All plots done Time to run AutoViz (in seconds) = 2.659 . Modelling . Aethos 2.0 introduces 3 new model objects: Classification, Regression and Unsupervised. These objects have the same capabilities of the Analysis object, but also can transform your data the same way it did in Aethos 1.0. For those new to Aethos, whenever you use Aethos to apply a transformation, it fits it to the training data and applies it to both the training and test data (in the case of Classification and Regression) to avoid data leakage. . In this post we&#39;ll cover the Classification object but the process is the exact same if you were working with a Regression or Unsupervised problem. . df = at.Classification(orig_data, target=&#39;Survived&#39;, test_split_percentage=.25) . As with Aethos 1.0 if no test data is provided, it is split upon initialization. In Aethos 2.0 it uses stratification for classification problems to split the data to ensure some resemblance of class balance. . . Warning: Earlier we showed the ability to alter the original dataframe and have it reflected in the Aethos object. This is NOT the case if you do not provide a test set for the Classification and Regression object. . df.describe() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . count 668 | 668 | 668 | NaN | NaN | 533 | 668 | 668 | NaN | 668 | NaN | NaN | . mean 441.913 | 0.383234 | 2.29192 | NaN | NaN | 29.4192 | 0.510479 | 0.377246 | NaN | 32.4659 | NaN | NaN | . std 260.048 | 0.486539 | 0.841285 | NaN | NaN | 14.7713 | 1.08757 | 0.781087 | NaN | 51.5116 | NaN | NaN | . min 1 | 0 | 1 | NaN | NaN | 0.42 | 0 | 0 | NaN | 0 | NaN | NaN | . 25% 214.75 | 0 | 1.75 | NaN | NaN | 20 | 0 | 0 | NaN | 7.925 | NaN | NaN | . 50% 450.5 | 0 | 3 | NaN | NaN | 28 | 0 | 0 | NaN | 14.4542 | NaN | NaN | . 75% 668.25 | 1 | 3 | NaN | NaN | 38 | 1 | 0 | NaN | 31.275 | NaN | NaN | . max 891 | 1 | 3 | NaN | NaN | 80 | 8 | 5 | NaN | 512.329 | NaN | NaN | . counts 668 | 668 | 668 | 668 | 668 | 533 | 668 | 668 | 668 | 668 | 160 | 666 | . uniques 668 | 2 | 3 | 668 | 2 | 82 | 7 | 6 | 545 | 216 | 121 | 3 | . missing 0 | 0 | 0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 508 | 2 | . missing_perc 0% | 0% | 0% | 0% | 0% | 20.21% | 0% | 0% | 0% | 0% | 76.05% | 0.30% | . types numeric | bool | numeric | unique | bool | numeric | numeric | numeric | categorical | numeric | categorical | categorical | . df.x_train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 482 | 0 | 2 | Frost, Mr. Anthony Wood &quot;Archie&quot; | male | NaN | 0 | 0 | 239854 | 0.0000 | NaN | S | . 1 828 | 1 | 2 | Mallet, Master. Andre | male | 1.0 | 0 | 2 | S.C./PARIS 2079 | 37.0042 | NaN | C | . 2 562 | 0 | 3 | Sivic, Mr. Husein | male | 40.0 | 0 | 0 | 349251 | 7.8958 | NaN | S | . 3 865 | 0 | 2 | Gill, Mr. John William | male | 24.0 | 0 | 0 | 233866 | 13.0000 | NaN | S | . 4 283 | 0 | 3 | de Pelsmaeker, Mr. Alfons | male | 16.0 | 0 | 0 | 345778 | 9.5000 | NaN | S | . df.x_test.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 187 | 1 | 3 | O&#39;Brien, Mrs. Thomas (Johanna &quot;Hannah&quot; Godfrey) | female | NaN | 1 | 0 | 370365 | 15.5000 | NaN | Q | . 1 321 | 0 | 3 | Dennis, Mr. Samuel | male | 22.0 | 0 | 0 | A/5 21172 | 7.2500 | NaN | S | . 2 379 | 0 | 3 | Betros, Mr. Tannous | male | 20.0 | 0 | 0 | 2648 | 4.0125 | NaN | C | . 3 698 | 1 | 3 | Mullens, Miss. Katherine &quot;Katie&quot; | female | NaN | 0 | 0 | 35852 | 7.7333 | NaN | Q | . 4 509 | 0 | 3 | Olsen, Mr. Henry Margido | male | 28.0 | 0 | 0 | C 4001 | 22.5250 | NaN | S | . df.missing_values . Train set missing values. Total Percent . Cabin 508 | 76.05% | . Age 135 | 20.21% | . Embarked 2 | 0.30% | . Fare 0 | 0.00% | . Ticket 0 | 0.00% | . Parch 0 | 0.00% | . SibSp 0 | 0.00% | . Sex 0 | 0.00% | . Name 0 | 0.00% | . Pclass 0 | 0.00% | . Survived 0 | 0.00% | . PassengerId 0 | 0.00% | . | Test set missing values. Total Percent . Cabin 179 | 80.27% | . Age 42 | 18.83% | . Embarked 0 | 0.00% | . Fare 0 | 0.00% | . Ticket 0 | 0.00% | . Parch 0 | 0.00% | . SibSp 0 | 0.00% | . Sex 0 | 0.00% | . Name 0 | 0.00% | . Pclass 0 | 0.00% | . Survived 0 | 0.00% | . PassengerId 0 | 0.00% | . | . . Tip: Aethos comes with a checklist to help give you reminders when cleaning, analyzing and transforming your data! . df.checklist() . df.standardize_column_names() . passengerid pclass name sex age sibsp parch ticket fare cabin embarked survived . 0 482 | 2 | Frost, Mr. Anthony Wood &quot;Archie&quot; | male | NaN | 0 | 0 | 239854 | 0.0000 | NaN | S | 0 | . 1 828 | 2 | Mallet, Master. Andre | male | 1.0 | 0 | 2 | S.C./PARIS 2079 | 37.0042 | NaN | C | 1 | . 2 562 | 3 | Sivic, Mr. Husein | male | 40.0 | 0 | 0 | 349251 | 7.8958 | NaN | S | 0 | . 3 865 | 2 | Gill, Mr. John William | male | 24.0 | 0 | 0 | 233866 | 13.0000 | NaN | S | 0 | . 4 283 | 3 | de Pelsmaeker, Mr. Alfons | male | 16.0 | 0 | 0 | 345778 | 9.5000 | NaN | S | 0 | . Since this is an overview, let&#39;s select the columns were going to work with and drop the ones we&#39;re not going to use. . df.drop(keep=[&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;fare&#39;, &#39;embarked&#39;]) . pclass sex age fare embarked survived . 0 2 | male | NaN | 0.0000 | S | 0 | . 1 2 | male | 1.0 | 37.0042 | C | 1 | . 2 3 | male | 40.0 | 7.8958 | S | 0 | . 3 2 | male | 24.0 | 13.0000 | S | 0 | . 4 3 | male | 16.0 | 9.5000 | S | 0 | . Let&#39;s chain our transformations together. Remember our transformations will be fit to the training data and automatically transform our test data! . is_child = lambda df: 1 if df[&#39;age&#39;] &lt; 18 else 0 df.replace_missing_median(&#39;age&#39;) .replace_missing_mostcommon(&#39;embarked&#39;) .onehot_encode(&#39;sex&#39;, &#39;pclass&#39;, &#39;embarked&#39;, keep_col=False) .apply(is_child, &#39;is_child&#39;) .normalize_numeric(&#39;fare&#39;, &#39;age&#39;) . Pandas Apply: 100%|██████████| 668/668 [00:00&lt;00:00, 77148.31it/s] Pandas Apply: 100%|██████████| 223/223 [00:00&lt;00:00, 57466.81it/s] . sex_female sex_male pclass_1 pclass_2 pclass_3 embarked_C embarked_Q embarked_S is_child fare age survived . 0 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.000000 | 0.346569 | 0 | . 1 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1 | 0.072227 | 0.007288 | 1 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.015412 | 0.497361 | 0 | . 3 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.025374 | 0.296306 | 0 | . 4 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1 | 0.018543 | 0.195778 | 0 | . df.x_train.head() . sex_female sex_male pclass_1 pclass_2 pclass_3 embarked_C embarked_Q embarked_S is_child fare age survived . 0 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.000000 | 0.346569 | 0 | . 1 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1 | 0.072227 | 0.007288 | 1 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.015412 | 0.497361 | 0 | . 3 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.025374 | 0.296306 | 0 | . 4 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1 | 0.018543 | 0.195778 | 0 | . df.x_test.head() . sex_female sex_male pclass_1 pclass_2 pclass_3 embarked_C embarked_Q embarked_S is_child fare age survived . 0 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0 | 0.030254 | 0.346569 | 1 | . 1 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.014151 | 0.271174 | 0 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0 | 0.007832 | 0.246042 | 0 | . 3 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0 | 0.015094 | 0.346569 | 1 | . 4 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.043966 | 0.346569 | 0 | . Now let&#39;s train a Logistic Regression model. . We&#39;ll use gridsearch and it will automatically return the best model. We&#39;ll use Stratified K-fold for the Cross Validation technique during grid search. . gs_params = { &quot;C&quot;: [0.1, 0.5, 1], &quot;max_iter&quot;: [100, 1000] } lr = df.LogisticRegression( cv_type=&#39;strat-kfold&#39;, gridsearch=gs_params, random_state=42 ) . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . Gridsearching with the following parameters: {&#39;C&#39;: [0.1, 0.5, 1], &#39;max_iter&#39;: [100, 1000]} Fitting 5 folds for each of 6 candidates, totalling 30 fits . [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 0.2s finished . LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . Once a model is trained a ModelAnalysis object is returned which allows us to analyze, interpret and visualize our model results. Included is a list to help you debug your model if it’s overfit or underfit! . df.help_debug() . You can quickly cross validate any model by calling cross_validate on the resulting ModelAnalysis object. It will display the mean score across all folds and a learning curve. . For classification problems the default cross validation method is Stratified K-Fold. This allows to maintain some form of class balance, while for regression, the default is K-Fold. . lr.cross_validate() . lr.metrics() # Note this displays the results on the test data. . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . Balanced Accuracy 0.774 | The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class. | . Average Precision 0.822 | Summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Zero One Loss 0.220 | Fraction of misclassifications. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . Recall 0.744 | It measures how many observations out of all positive observations have we classified as positive. Good to use when catching call positive occurences, usually at the cost of false positive. | . Matthews Correlation Coefficient 0.542 | It’s a correlation between predicted classes and ground truth. | . Log Loss 0.450 | Difference between ground truth and predicted score for every observation and average those errors over all observations. | . Jaccard 0.566 | Defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of true labels. | . Hinge Loss 0.511 | Computes the average distance between the model and the data using hinge loss, a one-sided metric that considers only prediction errors. | . Hamming Loss 0.220 | The Hamming loss is the fraction of labels that are incorrectly predicted. | . F-Beta 0.711 | It’s the harmonic mean between precision and recall, with an emphasis on one or the other. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . F1 0.723 | It’s the harmonic mean between precision and recall. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . Cohen Kappa 0.541 | Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies. Works well for imbalanced problems. | . Brier Loss 0.220 | It is a measure of how far your predictions lie from the true values. Basically, it is a mean square error in the probability space. | . Manual vs Automated . Lets&#39;s manually train a Logistic Regression and view and verify the results. . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score, precision_score X_train = df.x_train.drop(&quot;survived&quot;, axis=1) X_test = df.x_test.drop(&quot;survived&quot;, axis=1) y_train = df.x_train[&quot;survived&quot;] y_test = df.x_test[&quot;survived&quot;] clf = LogisticRegression(C=1, max_iter=100, random_state=42).fit(X_train, y_train) y_pred = clf.predict(X_test) print(f&quot;Accuracy: {accuracy_score(y_test, y_pred).round(3)}&quot;) print(f&quot;AUC: {roc_auc_score(y_test, clf.decision_function(X_test)).round(3)}&quot;) print(f&quot;Precision: {precision_score(y_test, y_pred).round(3)}&quot;) . Accuracy: 0.78 AUC: 0.853 Precision: 0.703 . Results are the same! . Model Analysis . Similar to Modelling, Aethos 2.0 introduces 4 model analysis objects: ClassificationModelAnalysis, RegressionModelAnalysis, UnsupervisedModelAnalysis and TextModelAnalysis. In Aethos 2.0 they can be initialized in 2 ways: . Result of training a model using Aethos . | Initializing it on your own by providing a Model object, the train data used by the model and the test data to evaluate model performance (for Regression and Classification). . | . Similar to the Model objects we&#39;re going to explore the ClassificationModelAnalysis object but the process would be the same for regression, unsupervised and text model analysis. . Initialzed from Aethos . To start, we&#39;ll pick off from where we left off with modelling and view the metrics for our Logistic Regression model. . type(lr) . aethos.model_analysis.classification_model_analysis.ClassificationModelAnalysis . lr.metrics() . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . Balanced Accuracy 0.774 | The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class. | . Average Precision 0.822 | Summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Zero One Loss 0.220 | Fraction of misclassifications. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . Recall 0.744 | It measures how many observations out of all positive observations have we classified as positive. Good to use when catching call positive occurences, usually at the cost of false positive. | . Matthews Correlation Coefficient 0.542 | It’s a correlation between predicted classes and ground truth. | . Log Loss 0.450 | Difference between ground truth and predicted score for every observation and average those errors over all observations. | . Jaccard 0.566 | Defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of true labels. | . Hinge Loss 0.511 | Computes the average distance between the model and the data using hinge loss, a one-sided metric that considers only prediction errors. | . Hamming Loss 0.220 | The Hamming loss is the fraction of labels that are incorrectly predicted. | . F-Beta 0.711 | It’s the harmonic mean between precision and recall, with an emphasis on one or the other. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . F1 0.723 | It’s the harmonic mean between precision and recall. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . Cohen Kappa 0.541 | Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies. Works well for imbalanced problems. | . Brier Loss 0.220 | It is a measure of how far your predictions lie from the true values. Basically, it is a mean square error in the probability space. | . You can also set project metrics based off your business requirements. . at.options.project_metrics = [&quot;Accuracy&quot;, &quot;ROC AUC&quot;, &quot;Precision&quot;] . lr.metrics() . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . If you want to just view individual metrics, there are functions for those to! . lr.fbeta(beta=0.4999) . 0.7111085827756309 . You can analyze any models results with just one line of code: . Metrics | Classification Report | Confusion Matrix | Decision Boundaries | Decision Plots | Dependence Plots | Force Plots | LIME Plots | Morris Sensitivity | Model Weights | Summary Plot | RoC Curve | Individual metrics | . And this is only for Classification Models, each type of problem has their own set of ModelAnalysis functions! . lr.classification_report() . precision recall f1-score support 0 0.83 0.80 0.82 137 1 0.70 0.74 0.72 86 accuracy 0.78 223 macro avg 0.77 0.77 0.77 223 weighted avg 0.78 0.78 0.78 223 . lr.confusion_matrix() . You can supply features from your train set to the dependency plot otherwise it will just use the first 2 features in your model. Under the hood it uses YellowBricks Decision Boundary visualizer to create the visualizations. . lr.decision_boundary(&#39;age&#39;, &#39;fare&#39;) . lr.decision_boundary() . Included are also automated SHAP use cases to interpret your model! . lr.decision_plot() . &lt;shap.plots.decision.DecisionPlotResult at 0x7f521b43abd0&gt; . lr.dependence_plot(&#39;age&#39;) . lr.force_plot() . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. lr.interpret_model() . 100%|██████████| 111/111 [00:00&lt;00:00, 617.89it/s] . Open in new window&lt;iframe src=&quot;http://127.0.0.1:7664/139990620277328/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; View the highest weighted features in your model. . lr.model_weights() . age : -1.64 sex_male : -1.23 sex_female : 1.23 pclass_3 : -1.06 pclass_1 : 1.05 is_child : 0.56 fare : 0.46 embarked_S : -0.33 embarked_C : 0.20 embarked_Q : 0.13 pclass_2 : 0.00 . Easily plot an RoC curve. . lr.roc_curve() . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f521b2e1f90&gt; . lr.summary_plot() . Finally we can generate the files to deploy our model through a RESTful API using FastAPI, Gunicorn and Docker! . lr.to_service(&#39;aethos2&#39;) . Deployment files can be found at /home/sidhu/.aethos/projects/aethos2. To run: docker build -t `image_name` ./ docker run -d --name `container_name` -p `port_num`:80 `image_name` . User Initialization . If we manually trained a model like we did earlier in the notebook and wanted to use Aethos&#39;s model analysis capabilties we can! . lr = at.ClassificationModelAnalysis( clf, df.x_train, df.x_test, target=&#39;survived&#39;, model_name=&#39;log_reg&#39; ) . . Note: x_train and x_test datasets must have the target variable as part of the DataFrame. . You will receive the same results as above, thus giving you the ability to manually transform your data, train your model and use Aethos to interpret the results. I&#39;ve included them below for verification. . lr.metrics() . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . lr.decision_boundary(&#39;age&#39;, &#39;fare&#39;) . lr.decision_boundary() . lr.decision_plot() . &lt;shap.plots.decision.DecisionPlotResult at 0x7f521cb2ed10&gt; . lr.dependence_plot(&#39;age&#39;) . lr.force_plot() . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. lr.interpret_model() . 100%|██████████| 111/111 [00:00&lt;00:00, 727.94it/s] . Open in new window&lt;iframe src=&quot;http://127.0.0.1:7664/139990657407168/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; lr.model_weights() . age : -1.64 sex_male : -1.23 sex_female : 1.23 pclass_3 : -1.06 pclass_1 : 1.05 is_child : 0.56 fare : 0.46 embarked_S : -0.33 embarked_C : 0.20 embarked_Q : 0.13 pclass_2 : 0.00 . lr.roc_curve() . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f521d910c90&gt; . lr.summary_plot()b . lr.to_service(&#39;aethos2&#39;) . Deployment files can be found at /home/sidhu/.aethos/projects/aethos2. To run: docker build -t `image_name` ./ docker run -d --name `container_name` -p `port_num`:80 `image_name` . Feedback . I encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. .",
            "url": "https://ashton-sidhu.github.io/blog/jupyter/aethos/datascience/2020/05/12/Aethos-2.0.html",
            "relUrl": "/jupyter/aethos/datascience/2020/05/12/Aethos-2.0.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
            "content": "Working on your own data science projects are a great opportunity to learn some new skills and hone existing skills, but what if you want to use technologies that you would use in industry such as Hadoop, Spark on a distributed cluster, Hive, etc. and have them all integrated? This is where the value comes from when building your own infrastructure. . You become familiar with the technologies, get to know the ins and outs about how it operates, debug and experience the different types of error messages and really get a sense of how the technology works over all instead of just interfacing with it. If you are also working with your own private data or confidential data in general, you may not want to upload it to an external service to do big data processing for privacy or security reasons. So, in this tutorial I’m going to walk through how to setup your own Big Data infrastructure on your own computer, home lab, etc. We’re going to setup a single node Hadoop &amp; Hive instance and a “distributed” spark cluster integrated with Jupyter. . Edit: Thanks to @Daniel Villanueva you can now deploy a VM with Hadoop, Spark and Hive pre-configured and ready to go through his Vagrant image. You can check it out on his Github here. . . This tutorial is not for an industry production installation! Prerequisites . A Debian based distro - Ubuntu, Pop-os, etc | Basic command line knowledge helps, but not essential for installation | . Step 1 - Download Hadoop and Hive . Hadoop is easily the most common big data warehouse platform used in industry today and is a must know for any big data job. In short, Hadoop is an open-source software framework used for storing and processing Big Data in a distributed manner. You can download the latest version from here. . Hive is usually added on top of Hadoop to query the data in Hadoop in a SQL like fashion. Hive makes job easy for performing operations like . Data encapsulation | Ad-hoc queries | Analysis of huge datasets | . Hive is slow and generally used for batch jobs only. A much faster version of Hive would be something like Impala, but for home use - it gets the job done. You can download the latest version of Hive here. . . Make sure you download the binary (bin) version and not the source (src) version! Extract the files to /opt . cd ~/Downloads tar -C /opt -xzvf apache-hive-3.1.2-bin.tar.gz tar -C /opt -xzvf hadoop-3.1.3-src.tar.gz . Rename them to hive and hadoop. . cd /opt mv hadoop-3.1.3-src hadoop mv apache-hive-3.1.2-bin hive . Step 2 - Setup Authorized (or Password-less) SSH. . Why do we need to do this? The Hadoop core uses Shell (SSH) to launch the server processes on the slave nodes. It requires a password-less SSH connection between the master and all conencted nodes. Otherwise, you would have to manually go to each node and start each Hadoop process. . Since we are running a local instance of Hadoop, we can save ourselves the hassle of setting up hostnames, SSH keys and adding them to each box. If this were a distributed environment, it would also be best to create a hadoop user, but it’s not necessary for a single node setup and personal use. . The really easy, only suitable for home use, should not be used or done anywhere else way is: . cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys . Now run ssh localhost and you should be able to login without a password. . . To get an idea of what it takes to configure the networking and infrastructure on a distributed environment, this is a great source. . Step 3 - Install Java 8 . One of the most important steps of this tutorial. . . If this is done incorrectly, it will cause a grueling number of hours debugging vague error messages just to realize the problem and solution was so simple. Hadoop has one main requirement and this is Java version 8. Funnily enough, that’s also the Java requirement for Spark, also very important. . sudo apt-get update sudo apt-get install openjdk-8-jdk . Verify the Java version. . java -version . . If for some reason you don’t see the output above, you need to update your default Java version. . sudo update-alternatives --config java . . Choose the number associated with Java 8. . Check the version again. . java -version . . Step 4 - Configure Hadoop + Yarn . Apache Hadoop YARN (Yet Another Resource Negotiator) is a cluster management technology. At a very basic level it helps Hadoop manage and monitor its workloads. . Initial Hadoop Setup . First let’s set our environment variables. These specifies where the configuration for Hadoop, Spark and Hive is located. . nano ~/.bashrc . Add this to the bottom of your .bashrc file. . export HADOOP_HOME=/opt/hadoop export HADOOP_INSTALL=$HADOOP_HOME export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH export HIVE_HOME=/opt/hive export PATH=$PATH:$HIVE_HOME/bin . Save and exit out of nano CTRL + o, CTRL + x. . Then we need to active these changes by running source ~/.bashrc. You can also close and reopen your terminal to achieve the same result. . Next we need to make some directories and edit permissions. Make the following directories: . sudo mkdir -p /app/hadoop/tmp mkdir -p ~/hdfs/namenode mkdir ~/hdfs/datanode . Edit the permissions for /app/hadoop/tmp, giving it read and write access. . sudo chown -R $USER:$USER /app chmod a+rw -R /app . Config Files . All the Hadoop configuration files are located in /opt/hadoop/etc/hadoop/. . cd /opt/hadoop/etc/hadoop . Next we need to edit the following configuration files: . - core-site.xml - hadoop-env.sh - hdfs-site.xml - mapred-site.xml - yarn-site.xml . core-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/hadoop/tmp&lt;/value&gt; &lt;description&gt;Parent directory for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS &lt;/name&gt; &lt;value&gt;hdfs://YOUR_IP:9000&lt;/value&gt; &lt;description&gt;The name of the default file system. &lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; . hadoop.tmp.dir: Fairly self explanatory, just a directory for hadoop to store other temp directories fs.defaultFS: The IP and port of your file system to access over the network. It should be your IP so other nodes can connect to it if this were a distributed system. . To find your ip, type ip addr or ifconfig on the command line: . . hadoop-env.sh . Identify the location of the Java 8 JDK, it should be similar or idential to /usr/lib/jvm/java-8-openjdk-amd64/ | Add the following line to hadoop-env.sh: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ | hdfs-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;Default block replication.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file:///home/YOUR_USER/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;file:///home/YOUR_USER/hdfs/datanode&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . dfs.replication: How many nodes to replicate the data on. . dfs.name.dir: Directory for namenode blocks . dfs.data.dir: Directory for the data node blocks . mapred-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.address&lt;/name&gt; &lt;value&gt;localhost:54311&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . mapreduce.framework.name: The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn. . mapreduce.jobtracker.address: The host and port that the MapReduce job tracker runs at. If “local”, then jobs are run in-process as a single map and reduce task. . yarn.app.mapreduce.am.env: Yarn map reduce env variable. . mapreduce.map.env: Map reduce map env variable. . mapreduce.reduce.env: Map reduce reduce env variable. . mapreduce.map.memory.mb: Upper memory limit that Hadoop allows to be allocated to a mapper, in megabytes. The default is 512. . mapreduce.reduce.memory.mb: Upper memory limit that Hadoop allows to be allocated to a reducer, in megabytes. The default is 512. . yarn-site.xml . &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;16256&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . yarn.resourcemanager.hostname: The hostname of the RM. Could also be an ip address of a remote yarn instance. . yarn.nodemanager.aux-services: Selects a shuffle service that needs to be set for MapReduce to run. . yarn.nodemanager.resource.memory-mb: Amount of physical memory, in MB, that can be allocated for containers. For reference, I have 64GB of RAM on my machine. If this value is too low, you won’t be able to process large files, getting a FileSegmentManagedBuffer error. . yarn.app.mapreduce.am.resource.mb: This property specify criteria to select resource for particular job. Any nodemanager which has equal or more memory available will get selected for executing job. . yarn.scheduler.minimum-allocation-mb: The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this won’t take effect, and the specified value will get allocated at minimum. . Start Hadoop . Before we start Hadoop we have to format the namenode: . hdfs namenode -format . Now we’re good to start Hadoop! Run the following commands: . start-dfs.sh start-yarn.sh . To ensure everything has started run the following commands: . ss -ln | grep 9000 . . jps . . You can now also access the Hadoop web UI at localhost:9870. . . You can also access the Yarn web UI at localhost:8088. . . Step 5 - Setup Hive . Now that we have Hadoop up and running, let’s install Hive on top of it. . First let’s make a directory in Hadoop where our Hive tables are going to be stored. . hdfs dfs -mkdir -p /user/hive/warehouse . Configure permissions. . hdfs dfs -chmod -R a+rw /user/hive . Setup a Metastore . The Hive Metastore is the central repository of Hive Metadata. It stores the meta data for Hive tables and relations (Schema and Locations etc). It provides client access to this information by using metastore service API. There are 3 different types of metastores: . Embedded Metastore: Only one Hive session can be open at a time. | Local Metastore: Multiple Hive sessions, have to connect to an external DB. | Remote Metastore: Multiple Hive sessions, interact with the metastore using Thrift API, better security and scalability. | . To read up on the difference between each type of metastore in more detail, this is a great link. . In this guide we’re going to be setting up a remote metastore using a MySQL DB. . sudo apt update sudo apt install mysql-server sudo mysql_secure_installation . Run the following commands: . sudo mysql . CREATE DATABASE metastore; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;PW_FOR_HIVE&#39;; GRANT ALL ON metastore.* TO &#39;hive&#39;@&#39;%&#39; WITH GRANT OPTION; . Replace PW_FOR_HIVE with the password you want to use for the hive user in MySQL. . Download the MySQL Java Connector: . wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19.tar.gz tar -xzvf mysql-connector-java-8.0.19.tar.gz cd mysql-connect-java-8.0.19 cp mysql-connector-java-8.0.19.jar /opt/hive/lib/ . Edit hive-site.xml . Now edit /opt/hive/conf/hive-site.xml: . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://YOUR_IP:3306/metastore?createDatabaseIfNotExist=true&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=UTC&lt;/value&gt; &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;MySQL JDBC driver class&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;user name for connecting to mysql server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;PW_FOR_HIVE&lt;/value&gt; &lt;description&gt;password for connecting to mysql server&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; . Replace YOUR_IP with the local ip address. Replace PW_FOR_HIVE with the password you initiated for the hive user earlier. . Initialize Schema . Now let’s make MySQL accessible from anywhere on your network. . sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf . Change bind-address to 0.0.0.0. . Restart the service for the changes take effect: sudo systemctl restart mysql.service . Finally, run schematool -dbType mysql -initSchema to initialize the schema in the metastore database. . Start Hive Metastore . hive --service metastore . Testing Hive . First start up Hive from the command line by calling hive. . Let’s create a test table: . CREATE TABLE IF NOT EXISTS test_table (col1 int COMMENT &#39;Integer Column&#39;, col2 string COMMENT &#39;String Column&#39;) COMMENT &#39;This is test table&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; STORED AS TEXTFILE; . Then insert some test data. . INSERT INTO test_table VALUES(1,&#39;aaa&#39;); . Then we can view the data from the table. . SELECT * FROM test_table; . Step 6 - Setup Spark . Spark is a general-purpose distributed data processing engine that is suitable for use in a wide range of circumstances. On top of the Spark core data processing engine, there are libraries for SQL, machine learning, graph computation, and stream processing, which can be used together in an application. In this tutorial we’re going to setup a standalone Spark cluster using Docker and have it be able to spin up any number of workers. This reasoning behind this is we want to simulate a remote cluster and some of the configuration required for it. . . In a production setting, Spark is usually going to be configured to use Yarn and the resources already allocated for Hadoop. First we need to create the Docker file. We’re going to use Spark version 2.4.4 in this tutorial but you can change it to 2.4.5 if you want the latest version and it also ships with Hadoop 2.7 to manage persistence and book keeping between nodes. In a production setting, Spark is often configured with Yarn to use the existing Hadoop environment and resources, since we only have Hadoop on one node, we’re going to run a spark standalone cluster. To configure Spark to run with Yarn requires minimal changes and you can see the difference in setup here. . Setup Standalone Cluster . nano Dockerfile . # Dockerfile FROM python:3.7-alpine ARG SPARK_VERSION=2.4.4 ARG HADOOP_VERSION=2.7 RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz &amp;&amp; tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C / &amp;&amp; rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz &amp;&amp; ln -s /spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark RUN apk add shell coreutils procps RUN apk fetch openjdk8 RUN apk add openjdk8 RUN pip3 install ipython ENV PYSPARK_DRIVER_PYTHON ipython . Now we want to spin up a Spark master and N number of spark workers. For this we’re going to use docker-compose. . nano docker-compose.yml . version: &quot;3.3&quot; networks: spark-network: services: spark-master: build: . container_name: spark-master hostname: spark-master command: &gt; /bin/sh -c &#39; /spark/sbin/start-master.sh &amp;&amp; tail -f /spark/logs/*&#39; ports: - 8080:8080 - 7077:7077 networks: - spark-network spark-worker: build: . depends_on: - spark-master command: &gt; /bin/sh -c &#39; /spark/sbin/start-slave.sh $$SPARK_MASTER &amp;&amp; tail -f /spark/logs/*&#39; env_file: - spark-worker.env environment: - SPARK_MASTER=spark://spark-master:7077 - SPARK_WORKER_WEBUI_PORT=8080 ports: - 8080 networks: - spark-network . For the master container we’re exposing port 7077 for our applications to connect to and port 8080 for the Spark job UI. For the worker we’re connecting to our Spark master through the environment variables. . For more options to configure the spark worker, we add them to spark-worker.env file. . nano spark-worker.env . SPARK_WORKER_CORES=3 SPARK_WORKER_MEMORY=8G . In this configuration, each worker will use 3 cores and have 8GB of memory. Since my machine has 6 cores, we’re going to start up 2 workers. Change these values so that it is relative to your machine. For example, if your machine only has 16GB of RAM, a good memory value might be 2 or 4GB. For a full list of environment variables and more information on stand alone mode, you can read the full documentation here. If you’re wondering about executor memory, that set when submitting or starting applications. . docker-compose build docker-compose up -d --scale spark-worker=2 . Now spark is up and running and you can view the web UI at localhost:8080! . . Install Spark Locally . On your local machine, or any machine that’s going to be creating or using Spark, Spark needs to be installed. Since we are setting up a remote Spark cluster we have install it from the source. We’re going to use PySpark for this tutorial because I most of the time I use Python for my personal projects. . You can download Spark from here. . . Ensure you download the same version you installed on your master. For this tutorial it&#39;s version 2.4.4 wget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz tar -C /opt -xzvf spark-2.4.4-bin-hadoop2.7.tgz . Setup the Spark environment variables, nano ~/.bashrc . export SPARK_HOME=/opt/spark export PATH=$SPARK_HOME/bin:$PATH export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot; export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot; export PYSPARK_PYTHON=python3 . . If you prefer Jupyter Lab, change &#39;notebook&#39;, to &#39;lab&#39; for PYSPARK_DRIVER_PYTHON_OPTS. Config Files . To configure Spark to use our Hadoop and Hive we need to have the config files for both in the Spark config folder. . cp $HADOOP_HOME/etc/hadoop/core-site.xml /opt/spark/conf/ cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml /opt/spark/conf/ . nano /opt/spark/conf/hive-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://YOUR_IP:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.sql.warehouse.dir&lt;/name&gt; &lt;value&gt;hdfs://YOUR_IP:9000/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . hive.metastore.uris: Tells Spark to interact with the Hive metastore using the Thrift API. spark.sql.warehouse.dir: Tells Spark where our Hive tables are located in HDFS. . Install PySpark . pip3 install pyspark==2.4.4 or replace 2.4.4 with whatever version you installed on your spark master. . To run PySpark connecting to our distributed cluster run: . pyspark --master spark://localhost:7077, you can also replace localhost with your ip or a remote ip. . This will start up a Jupyter Notebook with the Spark Context pre defined. As a result, we now have a single environment to analyze data with or without Spark. . By default the executor memory is only ~1GB (1024mb). To increase it start pyspark with the following command: . pyspark --master spark://localhost:7077 --executor-memory 7g . There is a 10% overhead per executor in Spark so the most we could assign is 7200mb, but to be safe and have a nice round number we’ll go with 7. . Test Integrations . By default a SparkContext is automatically created and the variable is sc. . To read from our previously created hive table. . from pyspark.sql import HiveContext hc = HiveContext(sc) hc.sql(&quot;show tables&quot;).show() hc.sql(&quot;select * from test_table&quot;).show() . To read a file from Hadoop the command would be: . sparksession = SparkSession.builder.appName(&quot;example-pyspark-read-and-write&quot;).getOrCreate() df = (sparksession .read .format(&quot;csv&quot;) .option(&quot;header&quot;, &quot;true&quot;) .load(&quot;hdfs://YOUR_IP:9000/PATH_TO_FILE&quot;) ) . Practical Hadoop Use Cases . Besides storing data, Hadoop is also utilized as a Feature Store. Let’s say you’re apart of a team or organization and they have multiple models. For each model there is a data pipeline that ingests raw data, computes and transforms the data into features. For one or two models this is perfectly fine, but what if you have multiple models? What if across those models features are being reused (i.e log normalized stock prices)? . Instead of each data pipeline recomputing the same features, we can create a data pipeline that computes the features once and store it in a Feature Store. The model can now pull features from the Feature Store without any redundant computation. This reduces the number of redundant computations and transformations throughout your data pipelines! . . Feature Stores also help with the following issues: . Features are not reused. A common obstacle data scientists face is spending time redeveloping features instead of using previously developed features or ones developed by other teams. Feature stores allow data scientists to avoid repeat work. . | Feature definitions vary. Different teams at any one company might define and name features differently. Moreover, accessing the documentation of a specific feature (if it exists at all) is often challenging. Feature stores address this issue by keeping features and their definitions organized and consistent. The documentation of the feature store helps you create a standardized language around all of the features across the company. You know exactly how every feature is computed and what information it represents. . | There is inconsistency between training and production features. Production and research environments often use different technologies and programming languages. The data streaming in to the production system needs to be processed into features in real time and fed into a machine learning model. . | . If you want to take a look at a Feature Store and get started for free, I recommend StreamSQL. StreamSQL allows you to stream your data from various sources such as HDFS, local file system, Kafka, etc. and create a data pipeline that can feed your model! It has the ability to save the feature store online or on your local HDFS for you to train your models. It also does the service of creating your test (hold out) set for you as well. They have a well documented API and is consistently improving upon it. . Feedback . I encourage all feedback about this post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help. . You can also reach me and follow me on Twitter at @ashtonasidhu. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/big%20data/hadoop/spark/hive/2020/04/17/big-data-setup.html",
            "relUrl": "/markdown/big%20data/hadoop/spark/hive/2020/04/17/big-data-setup.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Intro to Interactive Graph Visualizations",
            "content": "Over the last couple of months I have started to explore more with Graphs and network analysis as it pertains to user behaviour in cyber security. One of the main pain points I found early on was visualizing the network, the node properties and edges in a neat, visually appealing way. Alot of the default plotting options are built on Matplotlib, which is well, let&#39;s just say suboptimal for this type of visualization. . . Instead of playing around with the Matplotlib axes api, I wanted something a little bit more robust, easier to use out of the box, visually appealing and neat while maximizing the possible amount of information that can be displayed on the graph. . In comes my favourite visualization library Plotly. Plotly already had some documentation on how to visualize graphs, but it was still a fairly lengthy process. The goal was to find (or create) a uniform api that follows the .plot standard, with some customizability, but for graphs. Hence, why I made this little package called Interactive Graph Visualizations (igviz). . . Note: Currently, only Networkx is supported but the next step is expanding it to Apache&#8217;s Graphx. . To install run pip install igviz . import networkx as nx import igviz as ig . Create a random graph for demonstration purposes and assign every node a property called prop and every edge a property called &quot;edge_prop&quot; and make the values 12 and 3. . G = nx.random_geometric_graph(200, 0.125) nx.set_node_attributes(G, 12, &quot;prop&quot;) nx.set_edge_attributes(G, 3, &quot;edge_prop&quot;) . Now we plot! . Note: This also works with Directed and Multigraphs. Directed Graphs will show the arrows from node to node. . The Basics . . Tip: All the example notebooks can be found here. By default, the nodes are sized and coloured by the degree of itself. The degree of a node is just the number of edge&#39;s (lines connecting 2 nodes) it has. You can see the degree of the node by hovering over it! . fig = ig.plot(G) fig.show() . . . . Tip: When it comes to customizability, you can change the how the nodes are sized (size_method) to either be static, based off a node&#8217;s property or your own custom sizing method. . Tip: The color&#8217;s can be changed (color_method) to be a static color (hex, plain text, etc.), based off a node&#8217;s property or your own custom color method. . There are more options, and more to come, but those are the ones that greatly impact the visualization and information of your graph. . Customizing the Graph Appearance . Here all the nodes are set to the same size and the colour is set to a light red while displaying the prop property on hover. . . Tip: To display Node&#8217;s properties when you hover it, specify a list for the node_text parameter of node properties you want to have displayed. By default only degree is shown. . fig = ig.plot( G, # Your graph title=&quot;My Graph&quot;, size_method=&quot;static&quot;, # Makes node sizes the same color_method=&quot;#ffcccb&quot;, # Makes all the node colours black, node_text=[&quot;prop&quot;], # Adds the &#39;prop&#39; property to the hover text of the node annotation_text=&quot;Visualization made by &lt;a href=&#39;https://github.com/Ashton-Sidhu/plotly-graph&#39;&gt;igviz&lt;/a&gt; &amp; plotly.&quot;, # Adds a text annotation to the graph ) fig.show() . . . Here, the sizing and color method is based off the prop property of every node as well as we&#39;re displaying the prop property on hover. . fig = ig.plot( G, title=&quot;My Graph&quot;, size_method=&quot;prop&quot;, # Makes node sizes the size of the &quot;prop&quot; property color_method=&quot;prop&quot;, # Colors the nodes based off the &quot;prop&quot; property and a color scale, node_text=[&quot;prop&quot;], # Adds the &#39;prop&#39; property to the hover text of the node ) fig.show() . . . To add your own sizing and color methods, pass in a list of a color or size pertaining to each node in the graph to the size_method and color_method parameters. . Tip: To change the colorscale, change the colorscale parameter! . color_list = [] sizing_list = [] for node in G.nodes(): size_and_color = G.degree(node) * 3 color_list.append(size_and_color) sizing_list.append(size_and_color) fig = ig.plot( G, title=&quot;My Graph&quot;, size_method=sizing_list, # Makes node sizes the size of the &quot;prop&quot; property color_method=color_list, # Colors the nodes based off the &quot;prop&quot; property and a color scale, node_text=[&quot;prop&quot;], # Adds the &#39;prop&#39; property to the hover text of the node ) fig.show() . . . Labels . You can also add labels to both nodes and edges as well as add hover text to the edges based on their attributes. . ig.plot( G, node_label=&quot;prop&quot;, # Display the &quot;prop&quot; attribute as a label on the node node_label_position=&quot;top center&quot;, # Display the node label directly above the node edge_text=[&quot;edge_prop&quot;], # Display the &quot;edge_prop&quot; attribute on hover over the edge edge_label=&quot;edge_prop&quot;, # Display the &quot;edge_prop&quot; attribute on the edge edge_label_position=&quot;bottom center&quot;, # Display the edge label below the edge ) . . . Layouts . You can change the way your graph is oragnized and laid out by specifying a type of layout. Networkx comes with predefined layouts to use and we can apply them through layout. . By default, igviz looks for the pos node property and if it doesn&#39;t exist it will default to a random layout. . The supported layouts are: . random (default): Position nodes uniformly at random in the unit square. For every node, a position is generated by choosing each of dim coordinates uniformly at random on the interval [0.0, 1.0). . | circular: Position nodes on a circle. . | kamada: Position nodes using Kamada-Kawai path-length cost-function. . | planar: Position nodes without edge intersections, if possible (if the Graph is planar). . | spring: Position nodes using Fruchterman-Reingold force-directed algorithm. . | spectral: Position nodes using the eigenvectors of the graph Laplacian. . | spiral: Position nodes in a spiral layout. . | . fig = ig.plot( G, title=&quot;My Graph&quot;, layout=&quot;kamada&quot; ) fig.show() . . . To add your own pos property you can set it via the nx.set_node_attributes function. . pos_dict = { 0: [1, 2], # X, Y coordinates for Node 0 1: [1.5, 3], # X, Y coordinates for Node 1 ... } nx.set_node_attributes(G, pos_dict, &quot;pos&quot;) fig = ig.plot(G) fig.show() . Feedback . I encourage all feedback about this post or Igviz. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. . . Tip: If you like the project, give it a star! .",
            "url": "https://ashton-sidhu.github.io/blog/jupyter/graph/visualization/2020/03/27/intro-to-igviz.html",
            "relUrl": "/jupyter/graph/visualization/2020/03/27/intro-to-igviz.html",
            "date": " • Mar 27, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
            "content": "As a data scientist, rapid experimentation is extremely important. If an idea doesn’t work it’s best to fail quickly and find out sooner rather than later. . When it comes to modelling, rapid experimentation is already fairly simple. All model implementations follow the same API interface so all you have to do is initialize the model and train it. The problem comes now when you have to interpret, track, and compare each model. . Do you make a large notebook with all your models and scroll up and down or use a table of contents to see the results of different models? Do you create a different notebook for each model and then flip back and forth between notebooks? How do you track iterations of the models if you start tweaking the parameters? Where do you store artifacts to revisit at a later date or for further analysis? . I’m going to demonstrate a way to address these problems by training multiple models, each with 1 line of code, view the overall results easily, analyze the models, interpret the models, track the models in MLFlow and serve them using Aethos. . Prepping the Data . We’ll use Aethos to quickly prep the data. For more information on how to analyze and transform your datasets with Aethos, check out my previous blog post here. Load the Titanic training data from the Aethos repo. . import aethos as at import pandas as pd data = pd.read_csv(&#39;https://raw.githubusercontent.com/Ashton-Sidhu/aethos/develop/examples/data/train.csv&#39;) . Pass the data into Aethos. . df = at.Data(data, target_field=&#39;Survived&#39;) . . The focus of this post is modelling so let’s quickly preprocess the data. We’re going to use the Survived, Pclass, Sex, Age, Fare and Embarked features. insert here . df.drop(keep=[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]) df.standardize_column_names() . Replace the missing values in the Age and the Embarked columns. . df.replace_missing_median(&#39;age&#39;) df.replace_missing_mostcommon(&#39;embarked&#39;) . Normalize the values in the Age and Fare columns and One Hot Encode the Sex, Pclass and Embarked features. . df.onehot_encode(&#39;sex&#39;, &#39;pclass&#39;, &#39;embarked&#39;, keep_col=False) df.normalize_numeric(&#39;fare&#39;, &#39;age&#39;) . . With Aethos, the transformer is fit to the training set and applied to the test set. With just one line of code both your training and test set have been transformed. . Modelling . To train Sklearn, XGBoost, LightGBM, etc. models with Aethos, first transition from the data wrangling object to the Model object. . model = at.Model(df) . The model object behaves the same way as the Data object so if you have data that has already been processed, you can initiate the Model object the same way as the Data object. . Next, we’re going to enable experiment tracking with MLFlow. . at.options.track_experiments = True . Now that everything is set up, training a model and getting predictions is as simple as this: . lr = model.LogisticRegression(C=0.1) . . To train a model with the optimal parameters using Gridsearch, specify the gridsearch parameter when initializing the model. . lr = model.LogisticRegression(gridsearch={&#39;C&#39;: [0.01, 0.1]}, tol=0.001) . . This will return the model with the optimal parameters defined by your Gridsearch scoring method. . Finally, if you want to cross validate your models there are a few options: . lr = model.LogisticRegression(cv=5, C=0.001) . This will perform 5-fold cross validation on your model and display the mean score as well as the learning curve to help gauge data quality, over-fitting, and under-fitting. . . You can also use it with Gridsearch: . lr = model.LogisticRegression(cv=5, gridsearch={&#39;C&#39;: [0.01, 0.1]}, tol=0.001) . This will first use Gridsearch to train a model with the optimal parameters and then cross validate it. Currently supported cross validation methods are k-fold and stratified k-fold. . . To train multiple models at once (in series or in parallel) specify the run parameter when defining your model. . lr = model.LogisticRegression(cv=5, gridsearch={&#39;C&#39;: [0.01, 0.1]}, tol=0.001, run=False) . Let’s queue up a few more models to train: . model.DecisionTreeClassification(run=False) model.RandomForestClassification(run=False) model.LightGBMClassification(run=False) . You can view queued and trained models by running the following: . model.list_models() . . To now run all queued models, by default in parallel: . dt, rf, lgbm = model.run_models() . You can now go grab a coffee or a meal while all your models are trained simultaneously! . Analyzing Models . Every model you train by default has a name. This allows you to train multiple versions of the same model with the same model object and API, while still having access to each individual model’s results. You can see each model’s default name in the function header by pressing Shift + Tab in Jupyter Notebook. . . You can also change the model name by specifying a name of your choosing when initializing the function. . First, let’s compare all the models we’ve trained against each other: . model.compare_models() . . You can see every models’ performance against a wide variety of metrics. In a data science project there are predefined metrics you want to compare models against (if you don’t, you should). You can specify those project metrics through the options. . at.options.project_metrics = [&#39;Accuracy&#39;, &#39;Balanced Accuracy&#39;, &#39;Zero One Loss&#39;] . Now when you compare models, you’ll only see the project metrics. . model.compare_models() . . You can also view the metrics just for a single model by running the following: . dt.metrics() # Shows metrics for the Decision Tree model rf.metrics() # Shows metrics for the Random Forest model lgbm.metrics() # Shows metrics for the LightGBM model . You can use the same API to see each models’ RoC Curve, Confusion matrix, the index of misclassified predictions, etc. . dt.confusion_matrix(output_file=&#39;confusion_matrix.png&#39;) # Confusion matrix for the Decision Tree model . . rf.confusion_matrix(output_file=&#39;confusion_matrix.png&#39;) # Confusion matrix for the random forest model . . Interpreting Models . Aethos comes equipped with automated SHAP use cases to interpret each model. You can view the force, decision, summary, and dependence plot for any model — each with customizable parameters to satisfy your use case. . By specifying the output_file parameter, Aethos knows to save and track this artifact for a specific model. . lgbm.decision_plot(output_file=&#39;decision_plot.png&#39;) . . lgbm.force_plot(output_file=&#39;force_plot.png&#39;) . . lgbm.shap_get_misclassified_index() # [2, 10, 21, 38, 43, 57, 62, 69, 70, 85, 89, 91, 96, 98, 108, 117, 128, 129, 139, 141, 146, 156, 165, 167, 169] lgbm.force_plot(2, output_file=&#39;force_plot_2.png&#39;) . . lgbm.summary_plot(output_file=&#39;summary_plot.png&#39;) . . lgbm.dependence_plot(&#39;fare&#39;, output_file=&#39;dep_plot.png&#39;) . . For a more automated experience, run: . lgbm.interpret_model() . . This displays an interactive dashboard where you can interpret your model’s prediction using LIME, SHAP, Morris Sensitivity, etc. . Viewing Models in MLFlow . As we were training models and viewing results, our experiments were automatically being tracked. If you run aethos mlflow-ui from the command line, a local MLFlow instance will be started for you to view the results and artifacts of your experiments. Navigate to localhost:10000. . . Each of your models and any saved artifacts are tracked and can be viewed from the UI, including parameters and metrics! To view and download model artifacts, including the pickle file for a specific model, click on the hyperlink in the date column. . . You get a detailed breakdown of the metrics for the model as well as the model parameters and towards the bottom, you will see all your saved artifacts for a specific model. . . Note: Cross validation learning curves and mean score plots are always saved as artifacts. . To change the name of the experiment ( by default it is my-experiment) specify the name when initiating the Model object ( exp_name), otherwise every model will get added to my-experiment. . . Serving Models . Once you’ve decided on a model you want to serve for predictions, you can easily generate the required files to serve the model with a RESTful API using FastAPI and Gunicorn. . dt.to_service(&#39;titanic&#39;) . . If you’re familiar with MLFlow, you can always use it to serve your models as well. Open a terminal where the deployment files are being kept. I recommend moving these files to a git repository to version control both the model and served files. . Follow the instructions to build the docker container and then run it detached. . docker build -t titanic:1.0.0 ./ docker run -d --name titanic -p 1234:80 titanic:1.0.0 . Now navigate to localhost:1234/docs to test your API. You can now serve predictions by sending POST requests to 127.0.0.1:1234/predict. . . . Now one big thing to note, this is running a default configuration and should not be used in production without securing it and configuring the server for production use. In the future, I will add these configurations myself so you can more or less “throw” a model straight into production with minimal configuration changes. . This feature is still in its infancy and one that I will actively continue to develop and improve. . What Else Can You Do? . While this post is a semi- comprehensive guide of modelling with Aethos, you can also run statistical tests such as T-Test, Anovas, use pretrained models such as BERT and XLNet for sentiment analysis and question answering, perform extractive summarization with TextRank, train a gensim LDA model, as well as clustering, anomaly detection and regression models. . The full example can be seen here! . Feedback . I encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/aethos/2020/03/14/aethos-modelling.html",
            "relUrl": "/markdown/aethos/2020/03/14/aethos-modelling.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Aethos — A Data Science Library to Automate your Workflow",
            "content": "As a Data Scientist in industry, there are a few pain points that I’m sure a lot of other data scientists can relate to: . Copy and pasting code from previous projects to your current project or from notebook to notebook is frustrating. | For the same task, each data scientist will do it their own way. | Managing experiments with notebook names gets unorganized very quickly and structuring data science projects can be a mess. | Documenting your process while you’re in the middle of your analysis often disrupts your flow. | Version controlling models, transforming it into a service, and developing a data pipeline after a model has been trained. | There are tons of different libraries coming out that each fill a niche of Data Science &amp; ML and learning a new API (especially for deep learning and NLP) or reading documentation (if there is any) every time gets a little tiresome. Ironic, I know. | Making just even semi-appealing visualizations is often time consuming and sometimes difficult. | This is why I made Aethos. . Aethos is a Python library of automated Data Science techniques and use cases from missing value imputation, to NLP pre-processing, feature engineering, data visualizations and all the way to modelling and generating code to deploy your model as a service and soon, your data pipeline. . Installation . Let’s install Aethos like any other Python package: . pip install aethos . Once Aethos is installed, we can already do a couple of things. We can install all the required NLP corpora, . aethos install-corpora . install extensions such as QGrid for a better experience when interacting with your data . aethos enable-extensions . and create a Data Science project with the full folder structure to house source code, models, data and experiments. . aethos create . Getting Started . First, let’s import the necessary libraries: . import aethos as at import pandas as pd . Before we start, let’s also configure some options that will make our life easier. For myself, I often write a report of the steps I took during my analysis and why I did them to better communicate my process to other data scientists on the team or to myself if I come back to an experiment after a prolonged length of time. Instead of writing reports manually, Aethos will automatically write the report as a text file as we go through our analysis (all reports are saved in %HOME%/.aethos/reports/). To enable Aethos to write the report as a word doc, we just have to enable the option: . at.options.word_report = True at.options.interactive_table = True . The interactive_table option will display our data using the itables library, a personal favorite of mine as it has a client side search feature. . Now let’s load our data. Aethos takes in data in the form of a pandas dataframe so data is loaded just like if you were working with pandas. We’re going to use the titanic dataset for this example. . data = pd.read_csv(&#39;titanic.csv&#39;) . To use Aethos, we just have to pass the dataframe into the Aethos Data object. . df = at.Data(data, target_field=&#39;Survived&#39;, report_name=&#39;titanic&#39;) df . . Couple of things to note about what just happened: . Since this is a supervised learning problem we specified the column we want to predict (‘Survived’). | By default since we did not pass any test data to the Data object, Aethos automatically splits the data given into a train and test set. This can be turned off by setting split=False when initializing the Data object. | By default, the split percentage is 20. This can be changed by setting test_split_percentage to a float between 0 and 1 (by default it is 0.2). | We specified the report name to titanic, since we specified we want a Word Doc created, a titanic.txt and a titanic.docx will be created in %HOME%/.aethos/reports/. | . Let’s get started with some basic analysis. . Analysis . Using the Data object is just like working with a Pandas DataFrame. . df[&#39;Age&#39;] # returns a Pandas series of the training data in the Age column df[df[&#39;Age&#39;] &gt; 25] # returns a Pandas DataFrame of the the training data where Age is &gt; 25. df.nunique() # You can even run pandas functions on the Data object, however they only operate on your training data. . To add new columns is the same as doing it in pandas. When adding a new column it will add the new data to the dataset (train or test if data is split), based on the length of the dataset. You do, however, have the ability to specify which dataset to add it to as well: . # based off the length of some iterable it will add to the train set or test set (if data is split). df[`new_col_name`] = `some_iterable` # You can also specify what dataset to add it to as well df.x_train # pandas dataframe of the training set df.x_test # pandas dataframe of the test set df.x_train[`new_col`] = `some_iterable` df.x_test[`new_col`] = `some_iterable` . Aethos offers a few options to get started by getting a holistic view of your training data. The first one is one that you are probably familiar with. Aethos’ describe function extends pandas to provide a little extra information. . df.describe() . . We can also get more detailed information about each column, all of this is available through the pandas-summary library. . df.describe_column(&#39;Age&#39;) . . The command also provides a histogram of the data. Each statistical value can be accessed by referencing its name. . df.describe_column(&#39;Age&#39;) [&#39;iqr&#39; ] . The other option is to generate an EDA report available through pandas-profiling. . df.data_report() . . We can also generate histograms, pairplots and jointplots all with one line of code (Note: image sizes were scaled to a better fit for this article and are configurable). . df.jointplot(&#39;Age&#39;, &#39;Fare&#39;, kind=&#39;hex&#39;, output_file=&#39;age_fare_joint.png&#39;) df.pairplot(diag_kind=&#39;hist&#39;, output_file=&#39;pairplot.png&#39;) df.histogram(&#39;Age&#39;, output_file=&#39;age_hist.png&#39;) . . . Jointploit, Pairplot and histogram (L to R) . We can also view information about missing values in both datasets at any time. . df.missing_values . . Let’s deal with the missing values in the Cabin, Age and Embarked columns. . For the purpose of this article we will replace missing values in the Embarked column with the most common value and missing values in the Age column with the median value. We will then drop the Cabin column. . df.replace_missing_mostcommon(&#39;Embarked&#39;) . A lot just happened in a few lines of code. Let’s start with the imputation. . Aethos uses established packages (sci-kit learn, nltk, gensim, etc.) to do all the analysis, in this case, imputation. | To avoid data leakage all the analytical techniques are fit to the training set and then applied to the test set. | In general when there is a test set, whatever is done to the training set will be applied to the test set. | When you run a technique a small description of the technique is written to the report. | All of this is consistent for every Aethos analytical technique and can be viewed in the source code here (this function is in cleaning/clean.py). | . df.replace_missing_median(&#39;Age&#39;) df.drop(&#39;Cabin&#39;) . Let’s also drop the Ticket, Name, PClass, PassengerId, Parch, and SibSp columns. . df.drop(&#39;Ticket&#39;, &#39;Name&#39;, &#39;PClass&#39;, &#39;PassengerId&#39;, &#39;Parch&#39;, &#39;SibSp&#39;) . This isn’t a post about why you should try avoiding the .apply function in pandas but in the interest of API coverage and consistency, you can use .apply with Aethos as well. Specify a function and an output column name and you’re good to go. . The two big differences are that the whole DataFrame is passed into the function and the Swifter library is used to parallelize the running of the function, decreasing run time. . def get_person(data): age, sex = data[&#39;Age&#39;], data[&#39;Sex&#39;] return &#39;child&#39; if age &lt; 16 else sexdf.apply(get_person, &#39;Person&#39;) . . Just like with any other Aethos method the function get_person is applied to both the train and test set. Aethos provides an optimization when using .apply but it’s still best to use vectorization when possible. . We can now drop the Sex column. . df.drop(&#39;Sex&#39;) . We need to transform Embarked and Person to numeric variables and for this we’ll just use one-hot encoding for simplicity. We also want to drop the original columns which we specify with the keep_col parameter. Since we’re using scikit-learn’s OneHotEncoder class, we can pass keyword arguments to its’ constructor as well. . df.onehot_encode(&#39;Embarked&#39;, &#39;Person&#39;, keep_col=False, drop=None) . . Reporting . A couple of things to keep in mind about the current reporting feature is that it records and writes in the order the code is executed and that it currently does not caption any images. We can also write to the report at any time by doing the following: . df.log(`write_something_here`) . Conclusion . Aethos is standardized API that allows you to run analytical techniques, train models and more, with a single line of code. What I demonstrated is just the tip of the iceberg of the analytical techniques you can do with Aethos. There are over 110 automated techniques and models in Aethos with more coming soon. . In the subsequent blog posts I will go over training models and viewing results with Aethos. For more detail, I recommend reading the Usage section on Github or the documentation. . Coming Soon . This is just an introduction to Aethos but new features are coming soon! For a full roadmap you can view it here. . In the near future you can expect: . Experiment management with MLFlow | Improved existing visualizations with a better API | More automated visualizations | More automated techniques (dimensionality reduction, feature engineering) | Pre-trained models (GPT-2, BERT, etc.) | . Feedback . I encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/aethos/2020/03/14/aethos-intro.html",
            "relUrl": "/markdown/aethos/2020/03/14/aethos-intro.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "https://ashton-sidhu.github.io/blog/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://ashton-sidhu.github.io/blog/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ashton-sidhu.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ashton-sidhu.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://ashton-sidhu.github.io/blog/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . Hi! My name is Ashton Sidhu and I’m currently a Data Scientist focused in the cyber security area. I’m an engineer at heart and have both a Bachelors of Applied Science and Engineering as well as a Master’s of Engineering focused in Information Systems as well as Predictive and Perscriptive analytics. . I pride myself on being a swiss army knife engineer and being able to be self sufficient. I’ve been a software developer leading and delivering production releases of applications, I’ve also done ETL work, ingesting raw data and storing it into a Data Lake or Warehouse. I’ve also automated the deployment of infrastructure as well as deploy machine learning models into applications as well as production. I’ve always been naturally curious and had this drive be able to learn and do everything. It keeps me motivated to this day and allows me to experiment more freely as well as building my own software, application or product end to end. . Hobbies . Sports . Sports have always been a big part of my life since I was a kid, from playing high level baseball or sitting at home watching the Toronto Maple Leafs with the entire family (I used to cry when I was a kid when they lost in the playoffs). I played Baseball at the city level and pretty much everything else recreationally since I was a kid. . Nowadays, I find watching sports during the regular season difficult. As ironic as it may be, I think analytics has taken away the watchability of some sports. With hockey specifically, the game in the past used to be alot more free flowing and less structured compared to today’s games. I understand that this was going to be the natural evolution of sports as more data became accessible, but nonetheless, not the same as it used to be. Wow, am I getting old? . Played . Baseball: INF + Pitcher Hockey (Ball): Goalie + Defence Football: TE Soccer: Goalie Basketball: SG + SF Favorite Teams Baseball: Blue Jays Hockey: Toronto Maple Leafs Football: Oakland Raiders, Green Bay Packers (Aaron Rodgers), Kansas City Chiefs (Patrick Mahomes) Soccer: Barcelona, Bayern Munich, Liverpool Basketball: Toronto Raptors, Lebron James . Tech . I experiment with a lot of tech on my free time, specifically automation. I’ve set up my own home infrastructure to automate and aggegrate a lot of day to day tasks. I also experiment with infrastructure (I’m afraid to leave machines running on the cloud and get wopped with a huge bill.) and run a lot of my own on prem solutions. I run my own Spark cluster, data storages (Hadoop, Cassandra, ELK, etc.), Jupyter Server, Zeppelin, Kafka, etc for any analytics, monitoring, or ETL processes that I have throughout my place. . I also experiment with machine learning, specifically in the cyber security space, both on the defensive and offensive side. Those ideas can be seen and replicated through the blog posts on my site, so check them out. . Other than analytics, I also do some Hacker One as well as Hack the Box. . Gaming . I’ve been gaming since I was a kid on the original Gameboy playing Pokemon. I started as a PC Gamer with Runescape and Maplestory. Then moved to the PS3 to play CoD MW2 and then when League of Legends came out and took over the industry when I was in first year university .. well ya. I picked up CS: GO around 3rd year university, played in some amateur tournaments and took home some money. Then I retired from my E-sports career shortly after while on top… Anyways, now I just game casually on my free time. . Guild Wars 2 CS: GO Civilization 6 . Music . Who doesn’t like music? It’s fair to say that music plays a portion in everyone’s life whether it’s art or recreational. For me, studying music in university was my first option. When I first started even just in choir, I was terrible, always off key and couldn’t keep a rhythm to save my life. Then I picked up the Alto Sax, which I was just as terrible as when I first started. I was the last kid in my class to even just get a sound out of it. . Around Grade 9 is when everything changed and it turns out I had some skill and a passion for it. I made the decision that I wanted to study Performance Music in university. At this point I had made my mind up that I wanted to go the University of Toronto because at the time it was the best school in Canada (it might still be, I don’t keep up with the rankings anymore.). If I didn’t get into U of T for music, I was going to go there for Engineering Science. I managed to get the professor who teaches Alto Sax at U of T for Performance Music as my teacher and ultimately didn’t get in. . Since then, I’ve performed in 1 or 2 concerts and was the Alto Sax player for the engineering production Skule Nite for 5 years. Some of the greatest memories of my life. . Fun Facts . My first dream job was a Hockey commentator. I used to play mini sticks by myself and commentate the games. | I once serenaded McMaster university with “Careless Whisper”. There’s a video of it on the internet somewhere. | I was once a ranked chess player in Ontario. Used to compete in tournaments. | Latin music is my favourite genre. Although once I had an EDM phase. | Turns out, I’ve never travelled to a different timezone. It’s still one of my goals to see the world. | .",
          "url": "https://ashton-sidhu.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashton-sidhu.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}